{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytest_features.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFuYPBoE2O/C1EZXZZfGB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1dhiman/100days-ml/blob/master/2020/pytest_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_G6mKV4Q36Q",
        "colab_type": "text"
      },
      "source": [
        "# Pytest features\n",
        "\n",
        "## Testing for Exceptions\n",
        "You have a function that throws an exception and you want make sure that it happens under right conditions and includes correct message:\n",
        "\n",
        "Here we can see simple context manager that Pytest provides for us. It allows us to specify type of exception that should be raised as well as message of said exception. If the exception is not raised in the block, then test fails. You can also inspect more attributes of the exception as the context manager returns `ExceptionInfo` class that has attributes such as `type`, `value` or `traceback`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pjedo26Q2Wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytest\n",
        "\n",
        "def test_my_function():\n",
        "    with pytest.raises(Exception, match='My Message') as e:\n",
        "        my_function()\n",
        "        assert e.type is ValueError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLFu3mK-VSg7",
        "colab_type": "text"
      },
      "source": [
        "## Filtering Warnings\n",
        "With exceptions out of the way, let's look at warnings. Sometimes you get bunch of warning messages in your logs from inside of libraries that you use. You can't fix them and they really just create unnecessary noise, so let's get rid of them:\n",
        "\n",
        "Here we show 2 approaches - in first one, we straight up ignore all warnings of specified category by inserting a filter at the front of a filter list. This will cause your program to ignore all warnings of this category until it terminates, that might not always be desirable. \n",
        "\n",
        "With second approach, we use context manager that restores all warnings after exiting its scope. We also specify `record=True`, so that we can inspect list of issued (ignored) warnings if needs be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIizx8nYQ5bJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "from sqlalchemy.exc import SADeprecationWarning\n",
        "\n",
        "def test_my_function():\n",
        "    # SADeprecationWarning is issued by SQLAchemy when deprecated API is used\n",
        "    warnings.filterwarnings(\"ignore\", category=SADeprecationWarning)\n",
        "    ...\n",
        "    assert ...\n",
        "\n",
        "def test_my_function():\n",
        "    with warnings.catch_warnings(record=True) as w:\n",
        "        warnings.simplefilter(\"ignore\")  # ignore annoying warnings\n",
        "        ...\n",
        "        assert ...\n",
        "    # warnings restored"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCqDFjsvVt3l",
        "colab_type": "text"
      },
      "source": [
        "## Testing Standard Output and Standard Error Messages\n",
        "\n",
        "You have a command line tool that has bunch of functions that print messages to standard output but don't return anything. So, how do we test that?\n",
        "\n",
        "To solve this, Pytest provides fixture called `capsys`, which - well - captures system output. All you need to use it, is to add it as parameter to your test function. Next, after calling function that is being tested, you capture the outputs in form of tuple - `(out, err)`, which you can then use in assert statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTpu9j-fVmEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_my_function(capsys):\n",
        "    my_function()  # function that prints stuff\n",
        "    captured = capsys.readouterr()  # Capture output\n",
        "    assert f\"Received invalid message ...\" in captured.out  # Test stdout\n",
        "    assert f\"Fatal error ...\" in captured.err  # Test stderr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmC0dfGGWE5o",
        "colab_type": "text"
      },
      "source": [
        "## Patching Objects\n",
        "\n",
        "Sometimes when testing, you might need to replace objects used in functions under test to provide more predictable dataset or to avoid said function from accessing resources that might be unavailable. `mock.patch` can help with that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAsVeTu5WAZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from unittest import mock\n",
        "\n",
        "def test_my_function():\n",
        "    with mock.patch('module.some_function') as some_function:  # Used as context manager\n",
        "        my_function()  # function that calls `some_function`\n",
        "\n",
        "        some_function.assert_called_once()\n",
        "        some_function.assert_called_with(2, 'x')\n",
        "\n",
        "@mock.patch('module.func')  # Used as decorator\n",
        "def test_my_function(some_function):\n",
        "    module.func(10)  # Calls patched function\n",
        "    some_function.assert_called_with(10)  # True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOmynluIWZGd",
        "colab_type": "text"
      },
      "source": [
        "In this first example we can see that it's possible to patch functions and then check how many times and with what arguments they were called. These patches can also be stacked both in form of decorator and context manager. Now, for some more powerful uses:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7RPC1uwWTpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from unittest import mock\n",
        "\n",
        "def test_my_function():\n",
        "    with mock.patch.object(SomeClass, 'method_of_class', return_value=None) as mock_method:\n",
        "        instance = SomeClass()\n",
        "        instance.method_of_class('arg')\n",
        "\n",
        "        mock_method.assert_called_with('arg')  # True\n",
        "\n",
        "def test_my_function():\n",
        "    r = Mock()\n",
        "    r.content = b'{\"success\": true}'\n",
        "    with mock.patch('requests.get', return_value=r) as get:  # Avoid doing actual GET request\n",
        "        some_function()  # Function that calls requests.get\n",
        "        get.assert_called_once()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3dIDu_MWlvZ",
        "colab_type": "text"
      },
      "source": [
        "First example in above snippet is pretty straightforward - we replace method of `SomeClass` and make it return `None`. In the second, more practical example, we avoid being dependent on remote API/resource by replacing `requests.get` by mock and making it return object that we supply with suitable data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6d0JLrjW7LM",
        "colab_type": "text"
      },
      "source": [
        "## Sharing Fixtures with `conftest.py`\n",
        "\n",
        "If you write a lot of tests, then at some point you will realize that it would be nice to have all Pytest fixtures in one place, from which you would be able to import them and therefore share across test files. This can be solved with `conftest.py`.\n",
        "\n",
        "`conftest.py` is a file which resides at base of your test directory tree. In this file you can store all test fixtures and these are then automatically discovered by Pytest, so you don't even need to import them.\n",
        "\n",
        "This is also helpful if you need to share data between multiple tests - just create fixture that returns the test data.\n",
        "Another useful features is ability to specify scope of a fixture - this is important when you have fixtures that are very expensive to create, for example connections to database (session scope) and on other end of spectrum are the one that need to be reset after every test case (function scope). Possible values for fixture scope are: `function`, `class`, `module`, `package` and `session`.\n",
        "\n",
        "## Parametrizing Fixtures\n",
        "Above is an example of fixture that prepares SQLite testing database for each test. This fixture receives path to the database as parameter. This path is passed to the fixture using the `request` object, which attribute `param` is an iterable of all arguments passed to the fixture, in this case just one - the path. Here, this fixture first creates the database file (and could also populate it - omitted for clarity), then yields execution to the test and after test is finished, the fixture deletes the database file.\n",
        "\n",
        "As for the test itself, we use `@pytest.mark.parametrize` with 3 arguments - first of them is name of the fixture, second is a list of argument values for the fixture which will become the `request.param` and finally keyword argument `indirect=True`, which causes the argument values to appear in `request.param`.\n",
        "\n",
        "Last thing we need to do is add fixture as a argument to test itself and we are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcZl1XE6WbYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytest, os\n",
        "\n",
        "@pytest.fixture(scope='function')\n",
        "def reset_sqlite_db(request):\n",
        "    path = request.param  # Path to database file\n",
        "    with open(path, 'w'): pass\n",
        "    yield None\n",
        "    os.remove(path)\n",
        "\n",
        "@pytest.mark.parametrize('reset_sqlite_db', ['/tmp/test_db.sql'], indirect=True)\n",
        "def test_send_message(reset_sqlite_db):\n",
        "    ...  # Perform tests that access prepared SQLite database"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoGXutNMblzP",
        "colab_type": "text"
      },
      "source": [
        "## Skipping Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAmIJNpya_kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytest, os\n",
        "\n",
        "@pytest.mark.skipif(os.system(\"service postgresql status\") > 0,\n",
        "                    reason=\"PostgreSQL service is not running\")\n",
        "def test_connect_to_database():\n",
        "    ... # Run function that tries to connect to PostgreSQL database"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VLdXVN3b1x2",
        "colab_type": "text"
      },
      "source": [
        "[Source](https://martinheinz.dev/blog/7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfCG9iOqbp1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}