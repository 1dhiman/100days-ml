{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_Unsupervised_Learning_gans.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1dhiman/100days-ml/blob/master/2019/10_Unsupervised_Learning_gans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAjC2PhVZ1QR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Main'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time, re\n",
        "import pickle, gzip, datetime\n",
        "\n",
        "'''Data Viz'''\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.axes_grid1 import Grid\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "'''Data Prep and Model Evaluation'''\n",
        "from sklearn import preprocessing as pp\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "'''Algos'''\n",
        "import lightgbm as lgb\n",
        "\n",
        "'''TensorFlow and Keras'''\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.layers import LeakyReLU, Reshape, UpSampling2D, Conv2DTranspose\n",
        "from keras.layers import BatchNormalization, Input, Lambda\n",
        "from keras.layers import Embedding, Flatten, dot\n",
        "from keras import regularizers\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qnhm4uEahsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the datasets\n",
        "file = 'mnist.pkl.gz'\n",
        "f = gzip.open(file, 'rb')\n",
        "train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
        "f.close()\n",
        "\n",
        "X_train, y_train = train_set[0], train_set[1]\n",
        "X_validation, y_validation = validation_set[0], validation_set[1]\n",
        "X_test, y_test = test_set[0], test_set[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6ohiT8paq8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_keras = X_train.reshape(50000,28,28,1)\n",
        "X_validation_keras = X_validation.reshape(10000,28,28,1)\n",
        "X_test_keras = X_test.reshape(10000,28,28,1)\n",
        "\n",
        "y_train_keras = to_categorical(y_train)\n",
        "y_validation_keras = to_categorical(y_validation)\n",
        "y_test_keras = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8PCbY42ateZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Pandas DataFrames from the datasets\n",
        "train_index = range(0,len(X_train))\n",
        "validation_index = range(len(X_train),len(X_train)+len(X_validation))\n",
        "test_index = range(len(X_train)+len(X_validation),len(X_train)+ \\\n",
        "                   len(X_validation)+len(X_test))\n",
        "\n",
        "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
        "y_train = pd.Series(data=y_train,index=train_index)\n",
        "\n",
        "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
        "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
        "\n",
        "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
        "y_test = pd.Series(data=y_test,index=test_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP_DOLnlava3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def view_digit(X, y, example):\n",
        "    label = y.loc[example]\n",
        "    image = X.loc[example,:].values.reshape([28,28])\n",
        "    plt.title('Example: %d  Label: %d' % (example, label))\n",
        "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNunPpSRaxVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "2ed2beb4-fff4-4eb9-98ef-0baa28cddd9d"
      },
      "source": [
        "# View the first digit\n",
        "view_digit(X_train, y_train, 0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5fCbZer/ARUPJ6ewVFZ+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj7Z7DqMbYOB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a79b5a0-04ce-44b8-f4c4-54f0b2e58be8"
      },
      "source": [
        "# Confirm use of GPU\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else: print(\"Please install GPU version of TF\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default GPU Device: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikJJBlgFbY-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "f62f863c-1c49-4036-e447-abc1d3f0e1a3"
      },
      "source": [
        "# Convolutional Neural Network (CNN)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
        "                 activation ='relu', input_shape = (28,28,1)))\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation = \"softmax\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lLYcAtwbczA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e14ad161-97a2-4600-89b0-05bb01aed97a"
      },
      "source": [
        "# Train CNN\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "cnn_history = model.fit(X_train_keras, y_train_keras, \n",
        "          validation_data=(X_validation_keras, y_validation_keras), \\\n",
        "          epochs=100)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 28s 561us/step - loss: 0.1859 - acc: 0.9427 - val_loss: 0.0479 - val_acc: 0.9848\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 20s 409us/step - loss: 0.0697 - acc: 0.9795 - val_loss: 0.0335 - val_acc: 0.9897\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 20s 407us/step - loss: 0.0540 - acc: 0.9841 - val_loss: 0.0280 - val_acc: 0.9925\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 21s 411us/step - loss: 0.0438 - acc: 0.9867 - val_loss: 0.0340 - val_acc: 0.9905\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 20s 410us/step - loss: 0.0395 - acc: 0.9880 - val_loss: 0.0386 - val_acc: 0.9908\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 20s 409us/step - loss: 0.0335 - acc: 0.9895 - val_loss: 0.0286 - val_acc: 0.9922\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 20s 409us/step - loss: 0.0331 - acc: 0.9897 - val_loss: 0.0291 - val_acc: 0.9927\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 20s 405us/step - loss: 0.0290 - acc: 0.9909 - val_loss: 0.0287 - val_acc: 0.9920\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 20s 408us/step - loss: 0.0274 - acc: 0.9914 - val_loss: 0.0297 - val_acc: 0.9928\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 20s 405us/step - loss: 0.0250 - acc: 0.9924 - val_loss: 0.0251 - val_acc: 0.9932\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 21s 411us/step - loss: 0.0254 - acc: 0.9924 - val_loss: 0.0322 - val_acc: 0.9934\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 20s 404us/step - loss: 0.0230 - acc: 0.9931 - val_loss: 0.0279 - val_acc: 0.9936\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 20s 405us/step - loss: 0.0233 - acc: 0.9923 - val_loss: 0.0265 - val_acc: 0.9935\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 20s 402us/step - loss: 0.0213 - acc: 0.9935 - val_loss: 0.0362 - val_acc: 0.9924\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 20s 402us/step - loss: 0.0220 - acc: 0.9930 - val_loss: 0.0258 - val_acc: 0.9946\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 20s 401us/step - loss: 0.0225 - acc: 0.9928 - val_loss: 0.0299 - val_acc: 0.9932\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 20s 404us/step - loss: 0.0214 - acc: 0.9937 - val_loss: 0.0297 - val_acc: 0.9935\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.0213 - acc: 0.9936 - val_loss: 0.0332 - val_acc: 0.9933\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 20s 401us/step - loss: 0.0213 - acc: 0.9936 - val_loss: 0.0327 - val_acc: 0.9919\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0180 - acc: 0.9951 - val_loss: 0.0369 - val_acc: 0.9935\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0194 - acc: 0.9945 - val_loss: 0.0380 - val_acc: 0.9933\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0206 - acc: 0.9943 - val_loss: 0.0319 - val_acc: 0.9936\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0217 - acc: 0.9940 - val_loss: 0.0315 - val_acc: 0.9936\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.0171 - acc: 0.9947 - val_loss: 0.0292 - val_acc: 0.9937\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0198 - acc: 0.9943 - val_loss: 0.0310 - val_acc: 0.9933\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 20s 405us/step - loss: 0.0199 - acc: 0.9941 - val_loss: 0.0334 - val_acc: 0.9940\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0186 - acc: 0.9947 - val_loss: 0.0311 - val_acc: 0.9942\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.0199 - acc: 0.9946 - val_loss: 0.0369 - val_acc: 0.9930\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0203 - acc: 0.9944 - val_loss: 0.0321 - val_acc: 0.9939\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.0200 - acc: 0.9945 - val_loss: 0.0573 - val_acc: 0.9918\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0174 - acc: 0.9948 - val_loss: 0.0394 - val_acc: 0.9929\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0215 - acc: 0.9943 - val_loss: 0.0499 - val_acc: 0.9922\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0196 - acc: 0.9948 - val_loss: 0.0320 - val_acc: 0.9946\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 20s 401us/step - loss: 0.0187 - acc: 0.9950 - val_loss: 0.0274 - val_acc: 0.9941\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0169 - acc: 0.9952 - val_loss: 0.0343 - val_acc: 0.9938\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.0192 - acc: 0.9948 - val_loss: 0.0408 - val_acc: 0.9936\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.0159 - acc: 0.9959 - val_loss: 0.0422 - val_acc: 0.9929\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 20s 397us/step - loss: 0.0219 - acc: 0.9941 - val_loss: 0.0253 - val_acc: 0.9945\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 20s 403us/step - loss: 0.0194 - acc: 0.9948 - val_loss: 0.0320 - val_acc: 0.9935\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 20s 401us/step - loss: 0.0212 - acc: 0.9940 - val_loss: 0.0333 - val_acc: 0.9942\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0190 - acc: 0.9952 - val_loss: 0.0373 - val_acc: 0.9936\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 20s 404us/step - loss: 0.0181 - acc: 0.9954 - val_loss: 0.0436 - val_acc: 0.9936\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 20s 397us/step - loss: 0.0183 - acc: 0.9952 - val_loss: 0.0357 - val_acc: 0.9941\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0210 - acc: 0.9948 - val_loss: 0.0299 - val_acc: 0.9945\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0172 - acc: 0.9957 - val_loss: 0.0336 - val_acc: 0.9945\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 20s 397us/step - loss: 0.0185 - acc: 0.9954 - val_loss: 0.0452 - val_acc: 0.9932\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0171 - acc: 0.9955 - val_loss: 0.0446 - val_acc: 0.9930\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0356 - val_acc: 0.9934\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0170 - acc: 0.9953 - val_loss: 0.0363 - val_acc: 0.9947\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0261 - acc: 0.9935 - val_loss: 0.0396 - val_acc: 0.9932\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 0.0369 - val_acc: 0.9943\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0192 - acc: 0.9952 - val_loss: 0.0389 - val_acc: 0.9938\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0199 - acc: 0.9952 - val_loss: 0.0390 - val_acc: 0.9941\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0413 - val_acc: 0.9940\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 20s 402us/step - loss: 0.0178 - acc: 0.9957 - val_loss: 0.0403 - val_acc: 0.9943\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0195 - acc: 0.9954 - val_loss: 0.0420 - val_acc: 0.9940\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 20s 404us/step - loss: 0.0279 - acc: 0.9940 - val_loss: 0.0372 - val_acc: 0.9948\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.0167 - acc: 0.9958 - val_loss: 0.0471 - val_acc: 0.9938\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 20s 397us/step - loss: 0.0232 - acc: 0.9951 - val_loss: 0.0421 - val_acc: 0.9935\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 20s 397us/step - loss: 0.0173 - acc: 0.9962 - val_loss: 0.0452 - val_acc: 0.9944\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.0218 - acc: 0.9955 - val_loss: 0.0359 - val_acc: 0.9946\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0218 - acc: 0.9949 - val_loss: 0.0432 - val_acc: 0.9940\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 20s 397us/step - loss: 0.0227 - acc: 0.9950 - val_loss: 0.0370 - val_acc: 0.9950\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0201 - acc: 0.9955 - val_loss: 0.0399 - val_acc: 0.9944\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 0.0201 - acc: 0.9950 - val_loss: 0.0425 - val_acc: 0.9952\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 0.0188 - acc: 0.9957 - val_loss: 0.0600 - val_acc: 0.9923\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 0.0241 - acc: 0.9950 - val_loss: 0.0506 - val_acc: 0.9932\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 20s 391us/step - loss: 0.0194 - acc: 0.9957 - val_loss: 0.0454 - val_acc: 0.9940\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 0.0222 - acc: 0.9955 - val_loss: 0.0479 - val_acc: 0.9941\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.0189 - acc: 0.9957 - val_loss: 0.0408 - val_acc: 0.9948\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.0247 - acc: 0.9949 - val_loss: 0.0474 - val_acc: 0.9944\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.0249 - acc: 0.9950 - val_loss: 0.0408 - val_acc: 0.9943\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 20s 401us/step - loss: 0.0189 - acc: 0.9959 - val_loss: 0.0497 - val_acc: 0.9936\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0461 - val_acc: 0.9929\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.0211 - acc: 0.9957 - val_loss: 0.0532 - val_acc: 0.9937\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.0265 - acc: 0.9951 - val_loss: 0.0512 - val_acc: 0.9932\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.0232 - acc: 0.9952 - val_loss: 0.0423 - val_acc: 0.9944\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0233 - acc: 0.9952 - val_loss: 0.0381 - val_acc: 0.9950\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0272 - acc: 0.9951 - val_loss: 0.0450 - val_acc: 0.9948\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0342 - acc: 0.9946 - val_loss: 0.0492 - val_acc: 0.9941\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 0.0223 - acc: 0.9962 - val_loss: 0.0471 - val_acc: 0.9929\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.0251 - acc: 0.9954 - val_loss: 0.0502 - val_acc: 0.9938\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0213 - acc: 0.9955 - val_loss: 0.0535 - val_acc: 0.9942\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.0253 - acc: 0.9954 - val_loss: 0.0575 - val_acc: 0.9941\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.0308 - acc: 0.9948 - val_loss: 0.0469 - val_acc: 0.9939\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.0227 - acc: 0.9955 - val_loss: 0.0455 - val_acc: 0.9943\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0514 - val_acc: 0.9942\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.0259 - acc: 0.9953 - val_loss: 0.0477 - val_acc: 0.9937\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 20s 402us/step - loss: 0.0228 - acc: 0.9954 - val_loss: 0.0579 - val_acc: 0.9931\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0274 - acc: 0.9953 - val_loss: 0.0656 - val_acc: 0.9929\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0279 - acc: 0.9953 - val_loss: 0.0443 - val_acc: 0.9938\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0355 - acc: 0.9944 - val_loss: 0.0558 - val_acc: 0.9935\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.0254 - acc: 0.9952 - val_loss: 0.0553 - val_acc: 0.9933\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.0261 - acc: 0.9957 - val_loss: 0.0700 - val_acc: 0.9938\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.0233 - acc: 0.9956 - val_loss: 0.0561 - val_acc: 0.9933\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 20s 397us/step - loss: 0.0350 - acc: 0.9942 - val_loss: 0.0508 - val_acc: 0.9945\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.0322 - acc: 0.9945 - val_loss: 0.0522 - val_acc: 0.9939\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0350 - acc: 0.9945 - val_loss: 0.0467 - val_acc: 0.9946\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.0340 - acc: 0.9947 - val_loss: 0.0531 - val_acc: 0.9939\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 0.0311 - acc: 0.9951 - val_loss: 0.0497 - val_acc: 0.9950\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKtJqRSlbe1j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ada3b6e2-dced-4e8a-99a7-5b060322d6ed"
      },
      "source": [
        "print(cnn_history.history.keys())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjyT-Ht3bjrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "900e5d29-7f11-4ea6-df1a-be46c19e9686"
      },
      "source": [
        "# Plot Accuracy of CNN\n",
        "print(\"CNN Final Accuracy\", cnn_history.history['acc'][-1])\n",
        "pd.Series(cnn_history.history['acc']).plot(logy=False)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN Final Accuracy 0.99512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhIg/4/xzfNf+XbcTcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z68cH-cbk4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DCGAN\n",
        "\n",
        "class ElapsedTimer(object):\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "    def elapsed(self,sec):\n",
        "        if sec < 60:\n",
        "            return str(sec) + \" sec\"\n",
        "        elif sec < (60 * 60):\n",
        "            return str(sec / 60) + \" min\"\n",
        "        else:\n",
        "            return str(sec / (60 * 60)) + \" hr\"\n",
        "    def elapsed_time(self):\n",
        "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))\n",
        "\n",
        "class DCGAN(object):\n",
        "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channel = channel\n",
        "        self.D = None   # discriminator\n",
        "        self.G = None   # generator\n",
        "        self.AM = None  # adversarial model\n",
        "        self.DM = None  # discriminator model\n",
        "        \n",
        "    def generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8, \\\n",
        "                  window=5, input_dim=100, output_depth=1):\n",
        "        if self.G:\n",
        "            return self.G\n",
        "        self.G = Sequential()\n",
        "        self.G.add(Dense(dim*dim*depth, input_dim=input_dim))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "        self.G.add(Reshape((dim, dim, depth)))\n",
        "        self.G.add(Dropout(dropout))\n",
        "        \n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(Conv2DTranspose(output_depth, window, padding='same'))\n",
        "        self.G.add(Activation('sigmoid'))\n",
        "        self.G.summary()\n",
        "        return self.G\n",
        "\n",
        "    def discriminator(self, depth=64, dropout=0.3, alpha=0.3):\n",
        "        if self.D:\n",
        "            return self.D\n",
        "        self.D = Sequential()\n",
        "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
        "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\n",
        "            padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Flatten())\n",
        "        self.D.add(Dense(1))\n",
        "        self.D.add(Activation('sigmoid'))\n",
        "        self.D.summary()\n",
        "        return self.D\n",
        "\n",
        "    def discriminator_model(self):\n",
        "        if self.DM:\n",
        "            return self.DM\n",
        "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
        "        self.DM = Sequential()\n",
        "        self.DM.add(self.discriminator())\n",
        "        self.DM.compile(loss='binary_crossentropy', \\\n",
        "                        optimizer=optimizer, metrics=['accuracy'])\n",
        "        return self.DM\n",
        "\n",
        "    def adversarial_model(self):\n",
        "        if self.AM:\n",
        "            return self.AM\n",
        "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
        "        self.AM = Sequential()\n",
        "        self.AM.add(self.generator())\n",
        "        self.AM.add(self.discriminator())\n",
        "        self.AM.compile(loss='binary_crossentropy', \\\n",
        "                        optimizer=optimizer, metrics=['accuracy'])\n",
        "        return self.AM\n",
        "        \n",
        "class MNIST_DCGAN(object):\n",
        "    def __init__(self, x_train):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channel = 1\n",
        "\n",
        "        self.x_train = x_train\n",
        "\n",
        "        self.DCGAN = DCGAN()\n",
        "        self.discriminator =  self.DCGAN.discriminator_model()\n",
        "        self.adversarial = self.DCGAN.adversarial_model()\n",
        "        self.generator = self.DCGAN.generator()\n",
        "\n",
        "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
        "        noise_input = None\n",
        "        if save_interval>0:\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "        for i in range(train_steps):\n",
        "            images_train = self.x_train[np.random.randint(0,\n",
        "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            images_fake = self.generator.predict(noise)\n",
        "            x = np.concatenate((images_train, images_fake))\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "            \n",
        "            d_loss = self.discriminator.train_on_batch(x, y)\n",
        "\n",
        "            y = np.ones([batch_size, 1])\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], \\\n",
        "                                                      a_loss[1])\n",
        "            print(log_mesg)\n",
        "            if save_interval>0:\n",
        "                if (i+1)%save_interval==0:\n",
        "                    self.plot_images(save2file=True, \\\n",
        "                        samples=noise_input.shape[0],\\\n",
        "                        noise=noise_input, step=(i+1))\n",
        "\n",
        "    def plot_images(self, save2file=False, fake=True, samples=16, \\\n",
        "                    noise=None, step=0):\n",
        "        current_path = os.getcwd()\n",
        "        file = '\\\\images\\\\chapter12\\\\synthetic_mnist\\\\'\n",
        "        filename = 'mnist.png'\n",
        "        if fake:\n",
        "            if noise is None:\n",
        "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
        "            else:\n",
        "                filename = \"mnist_%d.png\" % step\n",
        "            images = self.generator.predict(noise)\n",
        "        else:\n",
        "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
        "            images = self.x_train[i, :, :, :]\n",
        "\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        if save2file:\n",
        "            plt.savefig(current_path+file+filename)\n",
        "            plt.close('all')\n",
        "        else:\n",
        "            plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YkI9gwrboq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d9d8d07-a028-4e53-a3ce-2c8421899025"
      },
      "source": [
        "# Initialize MNIST DCGAN and train\n",
        "mnist_dcgan = MNIST_DCGAN(X_train_keras)\n",
        "timer = ElapsedTimer()\n",
        "mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=500)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 8193      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 4,311,553\n",
            "Trainable params: 4,311,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 12544)             1266944   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 12544)             50176     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,394,241\n",
            "Trainable params: 2,368,705\n",
            "Non-trainable params: 25,536\n",
            "_________________________________________________________________\n",
            "0: [D loss: 0.696497, acc: 0.380859]  [A loss: 0.946863, acc: 0.000000]\n",
            "1: [D loss: 0.682668, acc: 0.500000]  [A loss: 0.867994, acc: 0.000000]\n",
            "2: [D loss: 0.626011, acc: 0.833984]  [A loss: 0.979623, acc: 0.000000]\n",
            "3: [D loss: 0.606653, acc: 0.613281]  [A loss: 1.000859, acc: 0.000000]\n",
            "4: [D loss: 0.548540, acc: 0.748047]  [A loss: 1.113635, acc: 0.000000]\n",
            "5: [D loss: 0.488167, acc: 0.871094]  [A loss: 0.482034, acc: 0.996094]\n",
            "6: [D loss: 2.462834, acc: 0.500000]  [A loss: 0.957966, acc: 0.000000]\n",
            "7: [D loss: 0.448959, acc: 0.953125]  [A loss: 0.915759, acc: 0.000000]\n",
            "8: [D loss: 0.565151, acc: 0.500000]  [A loss: 1.067583, acc: 0.000000]\n",
            "9: [D loss: 0.567517, acc: 0.500000]  [A loss: 1.113358, acc: 0.000000]\n",
            "10: [D loss: 0.543568, acc: 0.501953]  [A loss: 1.085187, acc: 0.000000]\n",
            "11: [D loss: 0.554338, acc: 0.500000]  [A loss: 1.104346, acc: 0.000000]\n",
            "12: [D loss: 0.522969, acc: 0.507812]  [A loss: 1.080742, acc: 0.000000]\n",
            "13: [D loss: 0.542545, acc: 0.505859]  [A loss: 1.174801, acc: 0.000000]\n",
            "14: [D loss: 0.496846, acc: 0.556641]  [A loss: 1.069208, acc: 0.000000]\n",
            "15: [D loss: 0.555263, acc: 0.500000]  [A loss: 1.444494, acc: 0.000000]\n",
            "16: [D loss: 0.448128, acc: 0.970703]  [A loss: 0.839449, acc: 0.070312]\n",
            "17: [D loss: 0.798511, acc: 0.500000]  [A loss: 1.657406, acc: 0.000000]\n",
            "18: [D loss: 0.521173, acc: 0.871094]  [A loss: 0.881861, acc: 0.019531]\n",
            "19: [D loss: 0.530005, acc: 0.503906]  [A loss: 1.122973, acc: 0.000000]\n",
            "20: [D loss: 0.426357, acc: 0.789062]  [A loss: 1.072891, acc: 0.000000]\n",
            "21: [D loss: 0.469356, acc: 0.570312]  [A loss: 1.287869, acc: 0.000000]\n",
            "22: [D loss: 0.395582, acc: 0.917969]  [A loss: 1.087013, acc: 0.000000]\n",
            "23: [D loss: 0.500545, acc: 0.556641]  [A loss: 1.588930, acc: 0.000000]\n",
            "24: [D loss: 0.434755, acc: 0.953125]  [A loss: 0.837785, acc: 0.132812]\n",
            "25: [D loss: 0.745666, acc: 0.500000]  [A loss: 1.658666, acc: 0.000000]\n",
            "26: [D loss: 0.503835, acc: 0.833984]  [A loss: 0.881372, acc: 0.046875]\n",
            "27: [D loss: 0.520385, acc: 0.521484]  [A loss: 1.175820, acc: 0.000000]\n",
            "28: [D loss: 0.385253, acc: 0.945312]  [A loss: 0.990666, acc: 0.023438]\n",
            "29: [D loss: 0.505115, acc: 0.544922]  [A loss: 1.368847, acc: 0.000000]\n",
            "30: [D loss: 0.386989, acc: 0.972656]  [A loss: 0.960067, acc: 0.023438]\n",
            "31: [D loss: 0.561722, acc: 0.517578]  [A loss: 1.505437, acc: 0.000000]\n",
            "32: [D loss: 0.419987, acc: 0.951172]  [A loss: 0.863113, acc: 0.140625]\n",
            "33: [D loss: 0.641614, acc: 0.501953]  [A loss: 1.474986, acc: 0.000000]\n",
            "34: [D loss: 0.423635, acc: 0.943359]  [A loss: 0.855494, acc: 0.132812]\n",
            "35: [D loss: 0.579530, acc: 0.513672]  [A loss: 1.354178, acc: 0.000000]\n",
            "36: [D loss: 0.398549, acc: 0.955078]  [A loss: 0.941434, acc: 0.035156]\n",
            "37: [D loss: 0.555245, acc: 0.533203]  [A loss: 1.394583, acc: 0.000000]\n",
            "38: [D loss: 0.421216, acc: 0.933594]  [A loss: 0.901784, acc: 0.132812]\n",
            "39: [D loss: 0.595064, acc: 0.523438]  [A loss: 1.465335, acc: 0.000000]\n",
            "40: [D loss: 0.424242, acc: 0.953125]  [A loss: 0.862342, acc: 0.125000]\n",
            "41: [D loss: 0.639392, acc: 0.507812]  [A loss: 1.481394, acc: 0.000000]\n",
            "42: [D loss: 0.453705, acc: 0.945312]  [A loss: 0.782386, acc: 0.304688]\n",
            "43: [D loss: 0.725282, acc: 0.501953]  [A loss: 1.438557, acc: 0.000000]\n",
            "44: [D loss: 0.461067, acc: 0.925781]  [A loss: 0.799346, acc: 0.226562]\n",
            "45: [D loss: 0.662898, acc: 0.505859]  [A loss: 1.309382, acc: 0.000000]\n",
            "46: [D loss: 0.467054, acc: 0.857422]  [A loss: 0.895271, acc: 0.132812]\n",
            "47: [D loss: 0.597771, acc: 0.515625]  [A loss: 1.341290, acc: 0.000000]\n",
            "48: [D loss: 0.474100, acc: 0.849609]  [A loss: 0.856273, acc: 0.160156]\n",
            "49: [D loss: 0.658884, acc: 0.503906]  [A loss: 1.458459, acc: 0.000000]\n",
            "50: [D loss: 0.480612, acc: 0.914062]  [A loss: 0.727710, acc: 0.414062]\n",
            "51: [D loss: 0.736994, acc: 0.500000]  [A loss: 1.434792, acc: 0.000000]\n",
            "52: [D loss: 0.507417, acc: 0.898438]  [A loss: 0.714179, acc: 0.476562]\n",
            "53: [D loss: 0.692894, acc: 0.501953]  [A loss: 1.344991, acc: 0.000000]\n",
            "54: [D loss: 0.499807, acc: 0.847656]  [A loss: 0.801430, acc: 0.257812]\n",
            "55: [D loss: 0.671496, acc: 0.503906]  [A loss: 1.379441, acc: 0.000000]\n",
            "56: [D loss: 0.515879, acc: 0.832031]  [A loss: 0.731379, acc: 0.398438]\n",
            "57: [D loss: 0.700606, acc: 0.501953]  [A loss: 1.399239, acc: 0.000000]\n",
            "58: [D loss: 0.525401, acc: 0.849609]  [A loss: 0.695506, acc: 0.496094]\n",
            "59: [D loss: 0.742612, acc: 0.500000]  [A loss: 1.405390, acc: 0.000000]\n",
            "60: [D loss: 0.525551, acc: 0.865234]  [A loss: 0.739039, acc: 0.433594]\n",
            "61: [D loss: 0.717008, acc: 0.503906]  [A loss: 1.400447, acc: 0.000000]\n",
            "62: [D loss: 0.536910, acc: 0.861328]  [A loss: 0.672052, acc: 0.574219]\n",
            "63: [D loss: 0.752975, acc: 0.500000]  [A loss: 1.443752, acc: 0.000000]\n",
            "64: [D loss: 0.547984, acc: 0.853516]  [A loss: 0.672728, acc: 0.609375]\n",
            "65: [D loss: 0.768515, acc: 0.500000]  [A loss: 1.420986, acc: 0.000000]\n",
            "66: [D loss: 0.560528, acc: 0.853516]  [A loss: 0.685632, acc: 0.535156]\n",
            "67: [D loss: 0.714552, acc: 0.501953]  [A loss: 1.322294, acc: 0.000000]\n",
            "68: [D loss: 0.556087, acc: 0.796875]  [A loss: 0.726835, acc: 0.414062]\n",
            "69: [D loss: 0.732460, acc: 0.500000]  [A loss: 1.452710, acc: 0.000000]\n",
            "70: [D loss: 0.570772, acc: 0.833984]  [A loss: 0.658798, acc: 0.597656]\n",
            "71: [D loss: 0.737188, acc: 0.500000]  [A loss: 1.377677, acc: 0.000000]\n",
            "72: [D loss: 0.558630, acc: 0.837891]  [A loss: 0.707769, acc: 0.488281]\n",
            "73: [D loss: 0.718601, acc: 0.500000]  [A loss: 1.425158, acc: 0.000000]\n",
            "74: [D loss: 0.550902, acc: 0.869141]  [A loss: 0.723313, acc: 0.386719]\n",
            "75: [D loss: 0.708862, acc: 0.500000]  [A loss: 1.457696, acc: 0.000000]\n",
            "76: [D loss: 0.559665, acc: 0.875000]  [A loss: 0.656018, acc: 0.617188]\n",
            "77: [D loss: 0.746120, acc: 0.500000]  [A loss: 1.465058, acc: 0.000000]\n",
            "78: [D loss: 0.559359, acc: 0.894531]  [A loss: 0.661188, acc: 0.625000]\n",
            "79: [D loss: 0.733920, acc: 0.500000]  [A loss: 1.381029, acc: 0.000000]\n",
            "80: [D loss: 0.562242, acc: 0.843750]  [A loss: 0.722429, acc: 0.382812]\n",
            "81: [D loss: 0.701293, acc: 0.500000]  [A loss: 1.398356, acc: 0.000000]\n",
            "82: [D loss: 0.551966, acc: 0.875000]  [A loss: 0.748497, acc: 0.332031]\n",
            "83: [D loss: 0.701236, acc: 0.500000]  [A loss: 1.496335, acc: 0.000000]\n",
            "84: [D loss: 0.558808, acc: 0.865234]  [A loss: 0.685837, acc: 0.523438]\n",
            "85: [D loss: 0.738450, acc: 0.500000]  [A loss: 1.495107, acc: 0.000000]\n",
            "86: [D loss: 0.562850, acc: 0.875000]  [A loss: 0.694843, acc: 0.476562]\n",
            "87: [D loss: 0.726005, acc: 0.500000]  [A loss: 1.424909, acc: 0.000000]\n",
            "88: [D loss: 0.565564, acc: 0.867188]  [A loss: 0.765396, acc: 0.277344]\n",
            "89: [D loss: 0.685394, acc: 0.501953]  [A loss: 1.420775, acc: 0.000000]\n",
            "90: [D loss: 0.551660, acc: 0.875000]  [A loss: 0.810076, acc: 0.175781]\n",
            "91: [D loss: 0.682584, acc: 0.501953]  [A loss: 1.505010, acc: 0.000000]\n",
            "92: [D loss: 0.560267, acc: 0.863281]  [A loss: 0.701390, acc: 0.480469]\n",
            "93: [D loss: 0.734838, acc: 0.500000]  [A loss: 1.636976, acc: 0.000000]\n",
            "94: [D loss: 0.591860, acc: 0.751953]  [A loss: 0.605626, acc: 0.804688]\n",
            "95: [D loss: 0.756506, acc: 0.500000]  [A loss: 1.404215, acc: 0.000000]\n",
            "96: [D loss: 0.577278, acc: 0.835938]  [A loss: 0.741382, acc: 0.390625]\n",
            "97: [D loss: 0.704205, acc: 0.500000]  [A loss: 1.387170, acc: 0.000000]\n",
            "98: [D loss: 0.587565, acc: 0.810547]  [A loss: 0.787604, acc: 0.230469]\n",
            "99: [D loss: 0.679357, acc: 0.505859]  [A loss: 1.435412, acc: 0.000000]\n",
            "100: [D loss: 0.571072, acc: 0.828125]  [A loss: 0.711301, acc: 0.433594]\n",
            "101: [D loss: 0.729566, acc: 0.500000]  [A loss: 1.583355, acc: 0.000000]\n",
            "102: [D loss: 0.592693, acc: 0.765625]  [A loss: 0.663209, acc: 0.605469]\n",
            "103: [D loss: 0.759726, acc: 0.500000]  [A loss: 1.466546, acc: 0.000000]\n",
            "104: [D loss: 0.584187, acc: 0.794922]  [A loss: 0.700773, acc: 0.507812]\n",
            "105: [D loss: 0.704215, acc: 0.500000]  [A loss: 1.372234, acc: 0.000000]\n",
            "106: [D loss: 0.562186, acc: 0.837891]  [A loss: 0.767024, acc: 0.296875]\n",
            "107: [D loss: 0.685429, acc: 0.500000]  [A loss: 1.512298, acc: 0.000000]\n",
            "108: [D loss: 0.571572, acc: 0.830078]  [A loss: 0.700638, acc: 0.500000]\n",
            "109: [D loss: 0.722727, acc: 0.500000]  [A loss: 1.611998, acc: 0.000000]\n",
            "110: [D loss: 0.586922, acc: 0.746094]  [A loss: 0.651427, acc: 0.648438]\n",
            "111: [D loss: 0.745451, acc: 0.498047]  [A loss: 1.422335, acc: 0.000000]\n",
            "112: [D loss: 0.586519, acc: 0.820312]  [A loss: 0.719845, acc: 0.457031]\n",
            "113: [D loss: 0.693779, acc: 0.500000]  [A loss: 1.396623, acc: 0.000000]\n",
            "114: [D loss: 0.567127, acc: 0.857422]  [A loss: 0.828655, acc: 0.136719]\n",
            "115: [D loss: 0.651997, acc: 0.501953]  [A loss: 1.509126, acc: 0.000000]\n",
            "116: [D loss: 0.574863, acc: 0.826172]  [A loss: 0.716179, acc: 0.425781]\n",
            "117: [D loss: 0.741992, acc: 0.500000]  [A loss: 1.660794, acc: 0.000000]\n",
            "118: [D loss: 0.591155, acc: 0.681641]  [A loss: 0.627286, acc: 0.707031]\n",
            "119: [D loss: 0.759255, acc: 0.500000]  [A loss: 1.396244, acc: 0.000000]\n",
            "120: [D loss: 0.591359, acc: 0.796875]  [A loss: 0.784379, acc: 0.250000]\n",
            "121: [D loss: 0.681235, acc: 0.501953]  [A loss: 1.359566, acc: 0.000000]\n",
            "122: [D loss: 0.568460, acc: 0.828125]  [A loss: 0.853246, acc: 0.109375]\n",
            "123: [D loss: 0.681656, acc: 0.505859]  [A loss: 1.547757, acc: 0.000000]\n",
            "124: [D loss: 0.579232, acc: 0.816406]  [A loss: 0.667352, acc: 0.589844]\n",
            "125: [D loss: 0.745702, acc: 0.500000]  [A loss: 1.577549, acc: 0.000000]\n",
            "126: [D loss: 0.587257, acc: 0.755859]  [A loss: 0.655886, acc: 0.625000]\n",
            "127: [D loss: 0.753316, acc: 0.500000]  [A loss: 1.426826, acc: 0.000000]\n",
            "128: [D loss: 0.593408, acc: 0.796875]  [A loss: 0.733531, acc: 0.339844]\n",
            "129: [D loss: 0.709662, acc: 0.500000]  [A loss: 1.437867, acc: 0.000000]\n",
            "130: [D loss: 0.590446, acc: 0.796875]  [A loss: 0.708840, acc: 0.484375]\n",
            "131: [D loss: 0.706559, acc: 0.500000]  [A loss: 1.481789, acc: 0.000000]\n",
            "132: [D loss: 0.596464, acc: 0.783203]  [A loss: 0.700540, acc: 0.488281]\n",
            "133: [D loss: 0.726215, acc: 0.498047]  [A loss: 1.533179, acc: 0.000000]\n",
            "134: [D loss: 0.593320, acc: 0.755859]  [A loss: 0.649658, acc: 0.636719]\n",
            "135: [D loss: 0.745360, acc: 0.500000]  [A loss: 1.471162, acc: 0.000000]\n",
            "136: [D loss: 0.595678, acc: 0.775391]  [A loss: 0.685341, acc: 0.546875]\n",
            "137: [D loss: 0.733998, acc: 0.500000]  [A loss: 1.455359, acc: 0.000000]\n",
            "138: [D loss: 0.595479, acc: 0.783203]  [A loss: 0.691491, acc: 0.511719]\n",
            "139: [D loss: 0.707745, acc: 0.501953]  [A loss: 1.433123, acc: 0.000000]\n",
            "140: [D loss: 0.595699, acc: 0.781250]  [A loss: 0.722130, acc: 0.425781]\n",
            "141: [D loss: 0.715937, acc: 0.500000]  [A loss: 1.445385, acc: 0.000000]\n",
            "142: [D loss: 0.588097, acc: 0.806641]  [A loss: 0.711678, acc: 0.476562]\n",
            "143: [D loss: 0.709321, acc: 0.501953]  [A loss: 1.501416, acc: 0.000000]\n",
            "144: [D loss: 0.593796, acc: 0.783203]  [A loss: 0.682212, acc: 0.542969]\n",
            "145: [D loss: 0.753513, acc: 0.496094]  [A loss: 1.508996, acc: 0.000000]\n",
            "146: [D loss: 0.609238, acc: 0.732422]  [A loss: 0.650366, acc: 0.640625]\n",
            "147: [D loss: 0.753162, acc: 0.500000]  [A loss: 1.435501, acc: 0.000000]\n",
            "148: [D loss: 0.593832, acc: 0.777344]  [A loss: 0.700672, acc: 0.507812]\n",
            "149: [D loss: 0.731815, acc: 0.496094]  [A loss: 1.341152, acc: 0.000000]\n",
            "150: [D loss: 0.598112, acc: 0.777344]  [A loss: 0.789264, acc: 0.273438]\n",
            "151: [D loss: 0.694675, acc: 0.503906]  [A loss: 1.447849, acc: 0.000000]\n",
            "152: [D loss: 0.581057, acc: 0.822266]  [A loss: 0.692063, acc: 0.535156]\n",
            "153: [D loss: 0.722568, acc: 0.498047]  [A loss: 1.538686, acc: 0.000000]\n",
            "154: [D loss: 0.586285, acc: 0.783203]  [A loss: 0.616768, acc: 0.734375]\n",
            "155: [D loss: 0.773281, acc: 0.498047]  [A loss: 1.503971, acc: 0.000000]\n",
            "156: [D loss: 0.605403, acc: 0.751953]  [A loss: 0.693047, acc: 0.542969]\n",
            "157: [D loss: 0.718155, acc: 0.501953]  [A loss: 1.405169, acc: 0.000000]\n",
            "158: [D loss: 0.605383, acc: 0.765625]  [A loss: 0.763285, acc: 0.347656]\n",
            "159: [D loss: 0.699392, acc: 0.501953]  [A loss: 1.382441, acc: 0.000000]\n",
            "160: [D loss: 0.591346, acc: 0.783203]  [A loss: 0.734607, acc: 0.347656]\n",
            "161: [D loss: 0.713951, acc: 0.503906]  [A loss: 1.582998, acc: 0.000000]\n",
            "162: [D loss: 0.585111, acc: 0.775391]  [A loss: 0.620873, acc: 0.718750]\n",
            "163: [D loss: 0.748464, acc: 0.501953]  [A loss: 1.507725, acc: 0.000000]\n",
            "164: [D loss: 0.598752, acc: 0.728516]  [A loss: 0.663806, acc: 0.570312]\n",
            "165: [D loss: 0.758595, acc: 0.500000]  [A loss: 1.426671, acc: 0.000000]\n",
            "166: [D loss: 0.611501, acc: 0.740234]  [A loss: 0.728642, acc: 0.421875]\n",
            "167: [D loss: 0.695345, acc: 0.507812]  [A loss: 1.379213, acc: 0.000000]\n",
            "168: [D loss: 0.587463, acc: 0.798828]  [A loss: 0.745256, acc: 0.363281]\n",
            "169: [D loss: 0.719892, acc: 0.505859]  [A loss: 1.563246, acc: 0.000000]\n",
            "170: [D loss: 0.602643, acc: 0.734375]  [A loss: 0.636059, acc: 0.683594]\n",
            "171: [D loss: 0.764447, acc: 0.500000]  [A loss: 1.474565, acc: 0.000000]\n",
            "172: [D loss: 0.610364, acc: 0.740234]  [A loss: 0.729916, acc: 0.410156]\n",
            "173: [D loss: 0.709385, acc: 0.503906]  [A loss: 1.410385, acc: 0.000000]\n",
            "174: [D loss: 0.608850, acc: 0.740234]  [A loss: 0.777930, acc: 0.308594]\n",
            "175: [D loss: 0.709810, acc: 0.507812]  [A loss: 1.389019, acc: 0.000000]\n",
            "176: [D loss: 0.600851, acc: 0.781250]  [A loss: 0.682118, acc: 0.535156]\n",
            "177: [D loss: 0.718241, acc: 0.503906]  [A loss: 1.505278, acc: 0.000000]\n",
            "178: [D loss: 0.604700, acc: 0.750000]  [A loss: 0.708038, acc: 0.445312]\n",
            "179: [D loss: 0.741728, acc: 0.509766]  [A loss: 1.477510, acc: 0.000000]\n",
            "180: [D loss: 0.607803, acc: 0.714844]  [A loss: 0.682997, acc: 0.570312]\n",
            "181: [D loss: 0.726910, acc: 0.500000]  [A loss: 1.470975, acc: 0.000000]\n",
            "182: [D loss: 0.603932, acc: 0.753906]  [A loss: 0.645793, acc: 0.636719]\n",
            "183: [D loss: 0.750428, acc: 0.501953]  [A loss: 1.402969, acc: 0.000000]\n",
            "184: [D loss: 0.613373, acc: 0.726562]  [A loss: 0.706579, acc: 0.464844]\n",
            "185: [D loss: 0.714562, acc: 0.501953]  [A loss: 1.264385, acc: 0.000000]\n",
            "186: [D loss: 0.613154, acc: 0.751953]  [A loss: 0.789310, acc: 0.289062]\n",
            "187: [D loss: 0.685759, acc: 0.521484]  [A loss: 1.431125, acc: 0.000000]\n",
            "188: [D loss: 0.602608, acc: 0.773438]  [A loss: 0.703885, acc: 0.472656]\n",
            "189: [D loss: 0.712675, acc: 0.513672]  [A loss: 1.495105, acc: 0.000000]\n",
            "190: [D loss: 0.619859, acc: 0.707031]  [A loss: 0.680486, acc: 0.546875]\n",
            "191: [D loss: 0.730844, acc: 0.500000]  [A loss: 1.413431, acc: 0.000000]\n",
            "192: [D loss: 0.625041, acc: 0.712891]  [A loss: 0.741425, acc: 0.414062]\n",
            "193: [D loss: 0.727313, acc: 0.501953]  [A loss: 1.462779, acc: 0.000000]\n",
            "194: [D loss: 0.630336, acc: 0.693359]  [A loss: 0.673439, acc: 0.605469]\n",
            "195: [D loss: 0.729036, acc: 0.501953]  [A loss: 1.414176, acc: 0.000000]\n",
            "196: [D loss: 0.619798, acc: 0.718750]  [A loss: 0.698946, acc: 0.500000]\n",
            "197: [D loss: 0.708037, acc: 0.498047]  [A loss: 1.340714, acc: 0.000000]\n",
            "198: [D loss: 0.607956, acc: 0.777344]  [A loss: 0.695529, acc: 0.507812]\n",
            "199: [D loss: 0.729769, acc: 0.496094]  [A loss: 1.357850, acc: 0.000000]\n",
            "200: [D loss: 0.636599, acc: 0.673828]  [A loss: 0.760762, acc: 0.335938]\n",
            "201: [D loss: 0.706524, acc: 0.501953]  [A loss: 1.335666, acc: 0.000000]\n",
            "202: [D loss: 0.628716, acc: 0.699219]  [A loss: 0.692851, acc: 0.523438]\n",
            "203: [D loss: 0.733984, acc: 0.500000]  [A loss: 1.377481, acc: 0.000000]\n",
            "204: [D loss: 0.634017, acc: 0.685547]  [A loss: 0.684146, acc: 0.562500]\n",
            "205: [D loss: 0.716391, acc: 0.509766]  [A loss: 1.289924, acc: 0.000000]\n",
            "206: [D loss: 0.624890, acc: 0.720703]  [A loss: 0.719925, acc: 0.441406]\n",
            "207: [D loss: 0.721674, acc: 0.509766]  [A loss: 1.379508, acc: 0.000000]\n",
            "208: [D loss: 0.629187, acc: 0.701172]  [A loss: 0.705355, acc: 0.468750]\n",
            "209: [D loss: 0.733068, acc: 0.500000]  [A loss: 1.398866, acc: 0.000000]\n",
            "210: [D loss: 0.632338, acc: 0.681641]  [A loss: 0.652824, acc: 0.644531]\n",
            "211: [D loss: 0.756978, acc: 0.500000]  [A loss: 1.332626, acc: 0.000000]\n",
            "212: [D loss: 0.636404, acc: 0.681641]  [A loss: 0.708010, acc: 0.484375]\n",
            "213: [D loss: 0.730048, acc: 0.505859]  [A loss: 1.327074, acc: 0.000000]\n",
            "214: [D loss: 0.642971, acc: 0.662109]  [A loss: 0.742731, acc: 0.406250]\n",
            "215: [D loss: 0.687687, acc: 0.511719]  [A loss: 1.266338, acc: 0.000000]\n",
            "216: [D loss: 0.638218, acc: 0.708984]  [A loss: 0.675716, acc: 0.558594]\n",
            "217: [D loss: 0.742981, acc: 0.498047]  [A loss: 1.356351, acc: 0.000000]\n",
            "218: [D loss: 0.650008, acc: 0.648438]  [A loss: 0.672663, acc: 0.578125]\n",
            "219: [D loss: 0.712055, acc: 0.500000]  [A loss: 1.297424, acc: 0.000000]\n",
            "220: [D loss: 0.641463, acc: 0.697266]  [A loss: 0.677184, acc: 0.597656]\n",
            "221: [D loss: 0.727506, acc: 0.496094]  [A loss: 1.267651, acc: 0.000000]\n",
            "222: [D loss: 0.669504, acc: 0.582031]  [A loss: 0.783364, acc: 0.261719]\n",
            "223: [D loss: 0.716587, acc: 0.500000]  [A loss: 1.295845, acc: 0.000000]\n",
            "224: [D loss: 0.637307, acc: 0.679688]  [A loss: 0.696831, acc: 0.476562]\n",
            "225: [D loss: 0.709184, acc: 0.503906]  [A loss: 1.310300, acc: 0.003906]\n",
            "226: [D loss: 0.653524, acc: 0.638672]  [A loss: 0.686821, acc: 0.496094]\n",
            "227: [D loss: 0.763316, acc: 0.503906]  [A loss: 1.430408, acc: 0.000000]\n",
            "228: [D loss: 0.645359, acc: 0.623047]  [A loss: 0.585726, acc: 0.812500]\n",
            "229: [D loss: 0.748734, acc: 0.494141]  [A loss: 1.216203, acc: 0.000000]\n",
            "230: [D loss: 0.662845, acc: 0.613281]  [A loss: 0.735224, acc: 0.378906]\n",
            "231: [D loss: 0.735029, acc: 0.500000]  [A loss: 1.202677, acc: 0.003906]\n",
            "232: [D loss: 0.650520, acc: 0.648438]  [A loss: 0.742121, acc: 0.382812]\n",
            "233: [D loss: 0.700609, acc: 0.507812]  [A loss: 1.173424, acc: 0.000000]\n",
            "234: [D loss: 0.647869, acc: 0.656250]  [A loss: 0.815150, acc: 0.195312]\n",
            "235: [D loss: 0.693138, acc: 0.503906]  [A loss: 1.221600, acc: 0.000000]\n",
            "236: [D loss: 0.638056, acc: 0.681641]  [A loss: 0.741398, acc: 0.390625]\n",
            "237: [D loss: 0.699802, acc: 0.513672]  [A loss: 1.356931, acc: 0.000000]\n",
            "238: [D loss: 0.637592, acc: 0.662109]  [A loss: 0.670394, acc: 0.593750]\n",
            "239: [D loss: 0.744873, acc: 0.500000]  [A loss: 1.548696, acc: 0.000000]\n",
            "240: [D loss: 0.660166, acc: 0.578125]  [A loss: 0.578332, acc: 0.828125]\n",
            "241: [D loss: 0.766597, acc: 0.498047]  [A loss: 1.280452, acc: 0.000000]\n",
            "242: [D loss: 0.670988, acc: 0.574219]  [A loss: 0.786354, acc: 0.246094]\n",
            "243: [D loss: 0.724623, acc: 0.498047]  [A loss: 1.199850, acc: 0.003906]\n",
            "244: [D loss: 0.661976, acc: 0.611328]  [A loss: 0.801820, acc: 0.226562]\n",
            "245: [D loss: 0.682299, acc: 0.519531]  [A loss: 1.041286, acc: 0.000000]\n",
            "246: [D loss: 0.661040, acc: 0.617188]  [A loss: 0.885084, acc: 0.074219]\n",
            "247: [D loss: 0.680897, acc: 0.541016]  [A loss: 1.151150, acc: 0.003906]\n",
            "248: [D loss: 0.654977, acc: 0.623047]  [A loss: 0.847643, acc: 0.136719]\n",
            "249: [D loss: 0.682773, acc: 0.523438]  [A loss: 1.332127, acc: 0.000000]\n",
            "250: [D loss: 0.642495, acc: 0.664062]  [A loss: 0.634678, acc: 0.734375]\n",
            "251: [D loss: 0.771984, acc: 0.503906]  [A loss: 1.550144, acc: 0.000000]\n",
            "252: [D loss: 0.672896, acc: 0.531250]  [A loss: 0.564281, acc: 0.878906]\n",
            "253: [D loss: 0.776389, acc: 0.500000]  [A loss: 1.160354, acc: 0.000000]\n",
            "254: [D loss: 0.671705, acc: 0.566406]  [A loss: 0.801008, acc: 0.187500]\n",
            "255: [D loss: 0.693933, acc: 0.527344]  [A loss: 1.052577, acc: 0.019531]\n",
            "256: [D loss: 0.664047, acc: 0.597656]  [A loss: 0.866655, acc: 0.070312]\n",
            "257: [D loss: 0.658374, acc: 0.582031]  [A loss: 0.927121, acc: 0.031250]\n",
            "258: [D loss: 0.663470, acc: 0.591797]  [A loss: 0.982245, acc: 0.015625]\n",
            "259: [D loss: 0.654503, acc: 0.628906]  [A loss: 0.973138, acc: 0.019531]\n",
            "260: [D loss: 0.650284, acc: 0.605469]  [A loss: 1.075579, acc: 0.023438]\n",
            "261: [D loss: 0.653228, acc: 0.625000]  [A loss: 0.886808, acc: 0.136719]\n",
            "262: [D loss: 0.694013, acc: 0.539062]  [A loss: 1.363760, acc: 0.000000]\n",
            "263: [D loss: 0.657662, acc: 0.621094]  [A loss: 0.601870, acc: 0.722656]\n",
            "264: [D loss: 0.761261, acc: 0.505859]  [A loss: 1.475170, acc: 0.000000]\n",
            "265: [D loss: 0.681317, acc: 0.535156]  [A loss: 0.609635, acc: 0.746094]\n",
            "266: [D loss: 0.799108, acc: 0.496094]  [A loss: 1.230830, acc: 0.000000]\n",
            "267: [D loss: 0.663132, acc: 0.603516]  [A loss: 0.684950, acc: 0.562500]\n",
            "268: [D loss: 0.692989, acc: 0.515625]  [A loss: 1.047865, acc: 0.003906]\n",
            "269: [D loss: 0.659423, acc: 0.648438]  [A loss: 0.763974, acc: 0.316406]\n",
            "270: [D loss: 0.699807, acc: 0.498047]  [A loss: 1.096354, acc: 0.003906]\n",
            "271: [D loss: 0.653014, acc: 0.623047]  [A loss: 0.762152, acc: 0.312500]\n",
            "272: [D loss: 0.684880, acc: 0.542969]  [A loss: 1.113846, acc: 0.003906]\n",
            "273: [D loss: 0.665891, acc: 0.623047]  [A loss: 0.755463, acc: 0.304688]\n",
            "274: [D loss: 0.708453, acc: 0.507812]  [A loss: 1.214143, acc: 0.000000]\n",
            "275: [D loss: 0.662124, acc: 0.615234]  [A loss: 0.698360, acc: 0.503906]\n",
            "276: [D loss: 0.714300, acc: 0.503906]  [A loss: 1.219125, acc: 0.000000]\n",
            "277: [D loss: 0.661810, acc: 0.630859]  [A loss: 0.652131, acc: 0.648438]\n",
            "278: [D loss: 0.718133, acc: 0.503906]  [A loss: 1.265922, acc: 0.000000]\n",
            "279: [D loss: 0.664778, acc: 0.574219]  [A loss: 0.634808, acc: 0.710938]\n",
            "280: [D loss: 0.749006, acc: 0.503906]  [A loss: 1.160508, acc: 0.000000]\n",
            "281: [D loss: 0.661013, acc: 0.593750]  [A loss: 0.753983, acc: 0.285156]\n",
            "282: [D loss: 0.690434, acc: 0.513672]  [A loss: 0.993512, acc: 0.011719]\n",
            "283: [D loss: 0.660255, acc: 0.628906]  [A loss: 0.813235, acc: 0.179688]\n",
            "284: [D loss: 0.681528, acc: 0.527344]  [A loss: 1.042742, acc: 0.003906]\n",
            "285: [D loss: 0.652243, acc: 0.650391]  [A loss: 0.803138, acc: 0.207031]\n",
            "286: [D loss: 0.677984, acc: 0.542969]  [A loss: 1.115159, acc: 0.003906]\n",
            "287: [D loss: 0.657898, acc: 0.634766]  [A loss: 0.756912, acc: 0.308594]\n",
            "288: [D loss: 0.705170, acc: 0.513672]  [A loss: 1.335495, acc: 0.000000]\n",
            "289: [D loss: 0.676953, acc: 0.580078]  [A loss: 0.616001, acc: 0.726562]\n",
            "290: [D loss: 0.751014, acc: 0.500000]  [A loss: 1.284492, acc: 0.000000]\n",
            "291: [D loss: 0.661028, acc: 0.572266]  [A loss: 0.636827, acc: 0.714844]\n",
            "292: [D loss: 0.716096, acc: 0.505859]  [A loss: 1.085335, acc: 0.000000]\n",
            "293: [D loss: 0.665915, acc: 0.599609]  [A loss: 0.734674, acc: 0.367188]\n",
            "294: [D loss: 0.684806, acc: 0.521484]  [A loss: 1.030885, acc: 0.003906]\n",
            "295: [D loss: 0.659209, acc: 0.617188]  [A loss: 0.774265, acc: 0.300781]\n",
            "296: [D loss: 0.676170, acc: 0.542969]  [A loss: 1.037961, acc: 0.007812]\n",
            "297: [D loss: 0.655030, acc: 0.648438]  [A loss: 0.794418, acc: 0.218750]\n",
            "298: [D loss: 0.694010, acc: 0.519531]  [A loss: 1.102354, acc: 0.000000]\n",
            "299: [D loss: 0.659943, acc: 0.613281]  [A loss: 0.729899, acc: 0.398438]\n",
            "300: [D loss: 0.700776, acc: 0.509766]  [A loss: 1.226049, acc: 0.000000]\n",
            "301: [D loss: 0.658313, acc: 0.623047]  [A loss: 0.680752, acc: 0.578125]\n",
            "302: [D loss: 0.723222, acc: 0.507812]  [A loss: 1.206368, acc: 0.003906]\n",
            "303: [D loss: 0.674182, acc: 0.570312]  [A loss: 0.634770, acc: 0.687500]\n",
            "304: [D loss: 0.726896, acc: 0.517578]  [A loss: 1.162903, acc: 0.000000]\n",
            "305: [D loss: 0.667109, acc: 0.617188]  [A loss: 0.668958, acc: 0.597656]\n",
            "306: [D loss: 0.709154, acc: 0.503906]  [A loss: 1.037267, acc: 0.011719]\n",
            "307: [D loss: 0.662479, acc: 0.642578]  [A loss: 0.742570, acc: 0.300781]\n",
            "308: [D loss: 0.685807, acc: 0.527344]  [A loss: 0.992827, acc: 0.019531]\n",
            "309: [D loss: 0.673225, acc: 0.585938]  [A loss: 0.785973, acc: 0.238281]\n",
            "310: [D loss: 0.690726, acc: 0.546875]  [A loss: 1.041598, acc: 0.011719]\n",
            "311: [D loss: 0.660479, acc: 0.634766]  [A loss: 0.800925, acc: 0.207031]\n",
            "312: [D loss: 0.674696, acc: 0.552734]  [A loss: 1.033338, acc: 0.011719]\n",
            "313: [D loss: 0.676893, acc: 0.582031]  [A loss: 0.822261, acc: 0.199219]\n",
            "314: [D loss: 0.695349, acc: 0.519531]  [A loss: 1.211356, acc: 0.023438]\n",
            "315: [D loss: 0.681414, acc: 0.552734]  [A loss: 0.810391, acc: 0.175781]\n",
            "316: [D loss: 0.687435, acc: 0.527344]  [A loss: 1.060269, acc: 0.027344]\n",
            "317: [D loss: 0.688167, acc: 0.533203]  [A loss: 0.862576, acc: 0.078125]\n",
            "318: [D loss: 0.688407, acc: 0.531250]  [A loss: 1.090632, acc: 0.003906]\n",
            "319: [D loss: 0.661765, acc: 0.621094]  [A loss: 0.753173, acc: 0.367188]\n",
            "320: [D loss: 0.695239, acc: 0.542969]  [A loss: 1.165191, acc: 0.000000]\n",
            "321: [D loss: 0.677333, acc: 0.572266]  [A loss: 0.641105, acc: 0.683594]\n",
            "322: [D loss: 0.742108, acc: 0.500000]  [A loss: 1.283734, acc: 0.000000]\n",
            "323: [D loss: 0.686616, acc: 0.529297]  [A loss: 0.617449, acc: 0.746094]\n",
            "324: [D loss: 0.735506, acc: 0.501953]  [A loss: 1.020799, acc: 0.000000]\n",
            "325: [D loss: 0.660034, acc: 0.638672]  [A loss: 0.729756, acc: 0.398438]\n",
            "326: [D loss: 0.709411, acc: 0.509766]  [A loss: 0.949829, acc: 0.031250]\n",
            "327: [D loss: 0.664721, acc: 0.603516]  [A loss: 0.775829, acc: 0.234375]\n",
            "328: [D loss: 0.676180, acc: 0.539062]  [A loss: 0.971183, acc: 0.007812]\n",
            "329: [D loss: 0.672066, acc: 0.568359]  [A loss: 0.771338, acc: 0.242188]\n",
            "330: [D loss: 0.691892, acc: 0.507812]  [A loss: 1.020432, acc: 0.003906]\n",
            "331: [D loss: 0.671511, acc: 0.636719]  [A loss: 0.763346, acc: 0.304688]\n",
            "332: [D loss: 0.687664, acc: 0.523438]  [A loss: 1.061061, acc: 0.011719]\n",
            "333: [D loss: 0.665966, acc: 0.595703]  [A loss: 0.695374, acc: 0.503906]\n",
            "334: [D loss: 0.742042, acc: 0.501953]  [A loss: 1.253495, acc: 0.000000]\n",
            "335: [D loss: 0.666379, acc: 0.562500]  [A loss: 0.634482, acc: 0.671875]\n",
            "336: [D loss: 0.733006, acc: 0.505859]  [A loss: 1.123236, acc: 0.000000]\n",
            "337: [D loss: 0.673629, acc: 0.574219]  [A loss: 0.704708, acc: 0.460938]\n",
            "338: [D loss: 0.736774, acc: 0.500000]  [A loss: 1.032826, acc: 0.011719]\n",
            "339: [D loss: 0.669632, acc: 0.582031]  [A loss: 0.718939, acc: 0.437500]\n",
            "340: [D loss: 0.697825, acc: 0.529297]  [A loss: 0.953089, acc: 0.027344]\n",
            "341: [D loss: 0.671861, acc: 0.582031]  [A loss: 0.777958, acc: 0.214844]\n",
            "342: [D loss: 0.690504, acc: 0.535156]  [A loss: 0.891880, acc: 0.105469]\n",
            "343: [D loss: 0.676089, acc: 0.587891]  [A loss: 0.871588, acc: 0.078125]\n",
            "344: [D loss: 0.680997, acc: 0.548828]  [A loss: 0.965579, acc: 0.058594]\n",
            "345: [D loss: 0.676315, acc: 0.580078]  [A loss: 0.841210, acc: 0.136719]\n",
            "346: [D loss: 0.682750, acc: 0.521484]  [A loss: 0.992104, acc: 0.007812]\n",
            "347: [D loss: 0.673117, acc: 0.605469]  [A loss: 0.809491, acc: 0.246094]\n",
            "348: [D loss: 0.718732, acc: 0.517578]  [A loss: 1.092838, acc: 0.003906]\n",
            "349: [D loss: 0.669441, acc: 0.621094]  [A loss: 0.678033, acc: 0.558594]\n",
            "350: [D loss: 0.703009, acc: 0.509766]  [A loss: 1.144073, acc: 0.000000]\n",
            "351: [D loss: 0.674202, acc: 0.568359]  [A loss: 0.657289, acc: 0.656250]\n",
            "352: [D loss: 0.718575, acc: 0.505859]  [A loss: 1.093122, acc: 0.007812]\n",
            "353: [D loss: 0.675944, acc: 0.566406]  [A loss: 0.693131, acc: 0.515625]\n",
            "354: [D loss: 0.716077, acc: 0.513672]  [A loss: 1.037078, acc: 0.000000]\n",
            "355: [D loss: 0.671678, acc: 0.605469]  [A loss: 0.736026, acc: 0.382812]\n",
            "356: [D loss: 0.705476, acc: 0.509766]  [A loss: 1.036818, acc: 0.003906]\n",
            "357: [D loss: 0.672719, acc: 0.582031]  [A loss: 0.731814, acc: 0.375000]\n",
            "358: [D loss: 0.686207, acc: 0.550781]  [A loss: 0.978959, acc: 0.015625]\n",
            "359: [D loss: 0.669206, acc: 0.601562]  [A loss: 0.770648, acc: 0.261719]\n",
            "360: [D loss: 0.683449, acc: 0.539062]  [A loss: 1.028716, acc: 0.011719]\n",
            "361: [D loss: 0.674065, acc: 0.585938]  [A loss: 0.773012, acc: 0.285156]\n",
            "362: [D loss: 0.689442, acc: 0.525391]  [A loss: 1.078131, acc: 0.007812]\n",
            "363: [D loss: 0.664892, acc: 0.636719]  [A loss: 0.676180, acc: 0.562500]\n",
            "364: [D loss: 0.721489, acc: 0.509766]  [A loss: 1.184330, acc: 0.000000]\n",
            "365: [D loss: 0.675655, acc: 0.558594]  [A loss: 0.606187, acc: 0.753906]\n",
            "366: [D loss: 0.730758, acc: 0.503906]  [A loss: 1.129616, acc: 0.000000]\n",
            "367: [D loss: 0.675501, acc: 0.550781]  [A loss: 0.690678, acc: 0.515625]\n",
            "368: [D loss: 0.706244, acc: 0.521484]  [A loss: 0.978497, acc: 0.011719]\n",
            "369: [D loss: 0.663890, acc: 0.634766]  [A loss: 0.757065, acc: 0.300781]\n",
            "370: [D loss: 0.685306, acc: 0.542969]  [A loss: 0.954086, acc: 0.011719]\n",
            "371: [D loss: 0.674630, acc: 0.580078]  [A loss: 0.780218, acc: 0.222656]\n",
            "372: [D loss: 0.687662, acc: 0.523438]  [A loss: 0.961798, acc: 0.027344]\n",
            "373: [D loss: 0.670042, acc: 0.625000]  [A loss: 0.782132, acc: 0.242188]\n",
            "374: [D loss: 0.674173, acc: 0.529297]  [A loss: 1.021888, acc: 0.003906]\n",
            "375: [D loss: 0.662643, acc: 0.636719]  [A loss: 0.737630, acc: 0.371094]\n",
            "376: [D loss: 0.705110, acc: 0.507812]  [A loss: 1.126511, acc: 0.011719]\n",
            "377: [D loss: 0.671415, acc: 0.583984]  [A loss: 0.680567, acc: 0.554688]\n",
            "378: [D loss: 0.737272, acc: 0.500000]  [A loss: 1.087609, acc: 0.007812]\n",
            "379: [D loss: 0.670078, acc: 0.605469]  [A loss: 0.707004, acc: 0.476562]\n",
            "380: [D loss: 0.700866, acc: 0.505859]  [A loss: 1.088943, acc: 0.007812]\n",
            "381: [D loss: 0.668362, acc: 0.599609]  [A loss: 0.666534, acc: 0.617188]\n",
            "382: [D loss: 0.729352, acc: 0.503906]  [A loss: 1.021660, acc: 0.003906]\n",
            "383: [D loss: 0.673146, acc: 0.599609]  [A loss: 0.729731, acc: 0.406250]\n",
            "384: [D loss: 0.705694, acc: 0.517578]  [A loss: 1.008152, acc: 0.000000]\n",
            "385: [D loss: 0.684380, acc: 0.537109]  [A loss: 0.814667, acc: 0.164062]\n",
            "386: [D loss: 0.690012, acc: 0.519531]  [A loss: 0.941498, acc: 0.027344]\n",
            "387: [D loss: 0.656391, acc: 0.638672]  [A loss: 0.773022, acc: 0.289062]\n",
            "388: [D loss: 0.692906, acc: 0.519531]  [A loss: 0.954700, acc: 0.015625]\n",
            "389: [D loss: 0.680301, acc: 0.562500]  [A loss: 0.822014, acc: 0.136719]\n",
            "390: [D loss: 0.676049, acc: 0.568359]  [A loss: 0.981794, acc: 0.074219]\n",
            "391: [D loss: 0.669208, acc: 0.587891]  [A loss: 0.778990, acc: 0.304688]\n",
            "392: [D loss: 0.705348, acc: 0.533203]  [A loss: 1.069534, acc: 0.007812]\n",
            "393: [D loss: 0.670305, acc: 0.595703]  [A loss: 0.749618, acc: 0.351562]\n",
            "394: [D loss: 0.702467, acc: 0.509766]  [A loss: 1.169871, acc: 0.000000]\n",
            "395: [D loss: 0.671009, acc: 0.566406]  [A loss: 0.623396, acc: 0.718750]\n",
            "396: [D loss: 0.740997, acc: 0.501953]  [A loss: 1.141722, acc: 0.003906]\n",
            "397: [D loss: 0.677984, acc: 0.546875]  [A loss: 0.658031, acc: 0.628906]\n",
            "398: [D loss: 0.710982, acc: 0.500000]  [A loss: 1.043293, acc: 0.007812]\n",
            "399: [D loss: 0.674265, acc: 0.570312]  [A loss: 0.721550, acc: 0.445312]\n",
            "400: [D loss: 0.694601, acc: 0.546875]  [A loss: 0.950577, acc: 0.031250]\n",
            "401: [D loss: 0.680242, acc: 0.546875]  [A loss: 0.773477, acc: 0.253906]\n",
            "402: [D loss: 0.681782, acc: 0.546875]  [A loss: 0.948063, acc: 0.015625]\n",
            "403: [D loss: 0.672205, acc: 0.597656]  [A loss: 0.788114, acc: 0.234375]\n",
            "404: [D loss: 0.690281, acc: 0.548828]  [A loss: 0.957835, acc: 0.007812]\n",
            "405: [D loss: 0.663897, acc: 0.623047]  [A loss: 0.747745, acc: 0.367188]\n",
            "406: [D loss: 0.699794, acc: 0.517578]  [A loss: 1.067021, acc: 0.011719]\n",
            "407: [D loss: 0.663834, acc: 0.630859]  [A loss: 0.727926, acc: 0.437500]\n",
            "408: [D loss: 0.718349, acc: 0.531250]  [A loss: 1.088079, acc: 0.003906]\n",
            "409: [D loss: 0.674815, acc: 0.607422]  [A loss: 0.661535, acc: 0.628906]\n",
            "410: [D loss: 0.727953, acc: 0.496094]  [A loss: 1.101121, acc: 0.019531]\n",
            "411: [D loss: 0.681219, acc: 0.562500]  [A loss: 0.713272, acc: 0.429688]\n",
            "412: [D loss: 0.691531, acc: 0.521484]  [A loss: 0.983924, acc: 0.031250]\n",
            "413: [D loss: 0.672457, acc: 0.582031]  [A loss: 0.780197, acc: 0.253906]\n",
            "414: [D loss: 0.677660, acc: 0.539062]  [A loss: 0.960979, acc: 0.019531]\n",
            "415: [D loss: 0.677187, acc: 0.578125]  [A loss: 0.793221, acc: 0.277344]\n",
            "416: [D loss: 0.678357, acc: 0.554688]  [A loss: 0.954211, acc: 0.027344]\n",
            "417: [D loss: 0.678737, acc: 0.566406]  [A loss: 0.778740, acc: 0.265625]\n",
            "418: [D loss: 0.682833, acc: 0.542969]  [A loss: 1.029964, acc: 0.015625]\n",
            "419: [D loss: 0.668607, acc: 0.593750]  [A loss: 0.771573, acc: 0.269531]\n",
            "420: [D loss: 0.682424, acc: 0.546875]  [A loss: 1.086174, acc: 0.003906]\n",
            "421: [D loss: 0.669591, acc: 0.601562]  [A loss: 0.682585, acc: 0.566406]\n",
            "422: [D loss: 0.732754, acc: 0.496094]  [A loss: 1.182850, acc: 0.007812]\n",
            "423: [D loss: 0.675726, acc: 0.560547]  [A loss: 0.704617, acc: 0.507812]\n",
            "424: [D loss: 0.691409, acc: 0.525391]  [A loss: 1.018317, acc: 0.023438]\n",
            "425: [D loss: 0.676389, acc: 0.546875]  [A loss: 0.741563, acc: 0.371094]\n",
            "426: [D loss: 0.695902, acc: 0.513672]  [A loss: 1.013107, acc: 0.000000]\n",
            "427: [D loss: 0.676306, acc: 0.566406]  [A loss: 0.779505, acc: 0.285156]\n",
            "428: [D loss: 0.675413, acc: 0.542969]  [A loss: 0.915587, acc: 0.062500]\n",
            "429: [D loss: 0.680893, acc: 0.550781]  [A loss: 0.819160, acc: 0.183594]\n",
            "430: [D loss: 0.679345, acc: 0.542969]  [A loss: 0.925321, acc: 0.035156]\n",
            "431: [D loss: 0.670797, acc: 0.580078]  [A loss: 0.874089, acc: 0.101562]\n",
            "432: [D loss: 0.675989, acc: 0.578125]  [A loss: 0.939070, acc: 0.050781]\n",
            "433: [D loss: 0.670399, acc: 0.595703]  [A loss: 0.947856, acc: 0.039062]\n",
            "434: [D loss: 0.661650, acc: 0.601562]  [A loss: 0.939967, acc: 0.066406]\n",
            "435: [D loss: 0.675381, acc: 0.574219]  [A loss: 0.930188, acc: 0.062500]\n",
            "436: [D loss: 0.669617, acc: 0.589844]  [A loss: 0.944878, acc: 0.066406]\n",
            "437: [D loss: 0.681148, acc: 0.589844]  [A loss: 0.936757, acc: 0.054688]\n",
            "438: [D loss: 0.679767, acc: 0.578125]  [A loss: 0.953518, acc: 0.050781]\n",
            "439: [D loss: 0.667600, acc: 0.568359]  [A loss: 0.938498, acc: 0.042969]\n",
            "440: [D loss: 0.676683, acc: 0.572266]  [A loss: 1.006883, acc: 0.054688]\n",
            "441: [D loss: 0.664593, acc: 0.609375]  [A loss: 0.831056, acc: 0.179688]\n",
            "442: [D loss: 0.705704, acc: 0.505859]  [A loss: 1.281952, acc: 0.011719]\n",
            "443: [D loss: 0.664642, acc: 0.599609]  [A loss: 0.649245, acc: 0.621094]\n",
            "444: [D loss: 0.735724, acc: 0.511719]  [A loss: 1.323276, acc: 0.000000]\n",
            "445: [D loss: 0.691094, acc: 0.523438]  [A loss: 0.606062, acc: 0.753906]\n",
            "446: [D loss: 0.778548, acc: 0.501953]  [A loss: 1.079330, acc: 0.003906]\n",
            "447: [D loss: 0.660620, acc: 0.599609]  [A loss: 0.715054, acc: 0.464844]\n",
            "448: [D loss: 0.693084, acc: 0.521484]  [A loss: 1.031616, acc: 0.007812]\n",
            "449: [D loss: 0.661605, acc: 0.589844]  [A loss: 0.754784, acc: 0.335938]\n",
            "450: [D loss: 0.684190, acc: 0.537109]  [A loss: 0.966036, acc: 0.042969]\n",
            "451: [D loss: 0.668367, acc: 0.621094]  [A loss: 0.778609, acc: 0.281250]\n",
            "452: [D loss: 0.693098, acc: 0.531250]  [A loss: 0.950898, acc: 0.050781]\n",
            "453: [D loss: 0.664243, acc: 0.601562]  [A loss: 0.846019, acc: 0.183594]\n",
            "454: [D loss: 0.684434, acc: 0.550781]  [A loss: 0.940943, acc: 0.058594]\n",
            "455: [D loss: 0.661225, acc: 0.626953]  [A loss: 0.821672, acc: 0.238281]\n",
            "456: [D loss: 0.670018, acc: 0.587891]  [A loss: 0.949600, acc: 0.066406]\n",
            "457: [D loss: 0.664238, acc: 0.603516]  [A loss: 0.872731, acc: 0.128906]\n",
            "458: [D loss: 0.677725, acc: 0.552734]  [A loss: 0.966064, acc: 0.031250]\n",
            "459: [D loss: 0.675550, acc: 0.568359]  [A loss: 0.861255, acc: 0.156250]\n",
            "460: [D loss: 0.689761, acc: 0.539062]  [A loss: 1.012311, acc: 0.035156]\n",
            "461: [D loss: 0.670580, acc: 0.599609]  [A loss: 0.873155, acc: 0.156250]\n",
            "462: [D loss: 0.667205, acc: 0.576172]  [A loss: 1.040203, acc: 0.031250]\n",
            "463: [D loss: 0.661090, acc: 0.623047]  [A loss: 0.763975, acc: 0.375000]\n",
            "464: [D loss: 0.703475, acc: 0.509766]  [A loss: 1.259005, acc: 0.007812]\n",
            "465: [D loss: 0.675068, acc: 0.558594]  [A loss: 0.588698, acc: 0.765625]\n",
            "466: [D loss: 0.737350, acc: 0.498047]  [A loss: 1.192390, acc: 0.000000]\n",
            "467: [D loss: 0.673740, acc: 0.570312]  [A loss: 0.665240, acc: 0.578125]\n",
            "468: [D loss: 0.703741, acc: 0.531250]  [A loss: 1.051912, acc: 0.042969]\n",
            "469: [D loss: 0.672633, acc: 0.595703]  [A loss: 0.801267, acc: 0.230469]\n",
            "470: [D loss: 0.684251, acc: 0.558594]  [A loss: 0.889524, acc: 0.136719]\n",
            "471: [D loss: 0.672973, acc: 0.564453]  [A loss: 0.864073, acc: 0.128906]\n",
            "472: [D loss: 0.688445, acc: 0.537109]  [A loss: 0.886237, acc: 0.128906]\n",
            "473: [D loss: 0.670748, acc: 0.585938]  [A loss: 0.899763, acc: 0.089844]\n",
            "474: [D loss: 0.669860, acc: 0.599609]  [A loss: 0.960708, acc: 0.066406]\n",
            "475: [D loss: 0.664636, acc: 0.599609]  [A loss: 0.813179, acc: 0.265625]\n",
            "476: [D loss: 0.681352, acc: 0.535156]  [A loss: 1.059625, acc: 0.011719]\n",
            "477: [D loss: 0.665203, acc: 0.583984]  [A loss: 0.785625, acc: 0.281250]\n",
            "478: [D loss: 0.696102, acc: 0.539062]  [A loss: 1.092191, acc: 0.011719]\n",
            "479: [D loss: 0.669399, acc: 0.597656]  [A loss: 0.709946, acc: 0.500000]\n",
            "480: [D loss: 0.694910, acc: 0.519531]  [A loss: 1.144257, acc: 0.015625]\n",
            "481: [D loss: 0.663452, acc: 0.617188]  [A loss: 0.690290, acc: 0.542969]\n",
            "482: [D loss: 0.702126, acc: 0.527344]  [A loss: 1.116411, acc: 0.000000]\n",
            "483: [D loss: 0.671910, acc: 0.591797]  [A loss: 0.681173, acc: 0.566406]\n",
            "484: [D loss: 0.724336, acc: 0.521484]  [A loss: 1.093429, acc: 0.007812]\n",
            "485: [D loss: 0.674072, acc: 0.570312]  [A loss: 0.706377, acc: 0.484375]\n",
            "486: [D loss: 0.689399, acc: 0.527344]  [A loss: 1.007169, acc: 0.031250]\n",
            "487: [D loss: 0.671338, acc: 0.597656]  [A loss: 0.720897, acc: 0.453125]\n",
            "488: [D loss: 0.701070, acc: 0.521484]  [A loss: 1.053990, acc: 0.019531]\n",
            "489: [D loss: 0.658595, acc: 0.617188]  [A loss: 0.747700, acc: 0.406250]\n",
            "490: [D loss: 0.699695, acc: 0.537109]  [A loss: 1.076581, acc: 0.003906]\n",
            "491: [D loss: 0.669681, acc: 0.599609]  [A loss: 0.744618, acc: 0.339844]\n",
            "492: [D loss: 0.697714, acc: 0.523438]  [A loss: 1.020732, acc: 0.023438]\n",
            "493: [D loss: 0.666820, acc: 0.593750]  [A loss: 0.715661, acc: 0.468750]\n",
            "494: [D loss: 0.704140, acc: 0.517578]  [A loss: 1.028889, acc: 0.011719]\n",
            "495: [D loss: 0.664194, acc: 0.603516]  [A loss: 0.742997, acc: 0.398438]\n",
            "496: [D loss: 0.700107, acc: 0.525391]  [A loss: 1.032918, acc: 0.039062]\n",
            "497: [D loss: 0.672960, acc: 0.572266]  [A loss: 0.741763, acc: 0.414062]\n",
            "498: [D loss: 0.691120, acc: 0.533203]  [A loss: 1.010415, acc: 0.054688]\n",
            "499: [D loss: 0.672699, acc: 0.580078]  [A loss: 0.847679, acc: 0.191406]\n",
            "500: [D loss: 0.694795, acc: 0.550781]  [A loss: 1.005350, acc: 0.015625]\n",
            "501: [D loss: 0.674399, acc: 0.574219]  [A loss: 0.783164, acc: 0.246094]\n",
            "502: [D loss: 0.690879, acc: 0.527344]  [A loss: 1.034240, acc: 0.031250]\n",
            "503: [D loss: 0.679752, acc: 0.582031]  [A loss: 0.737767, acc: 0.378906]\n",
            "504: [D loss: 0.686562, acc: 0.535156]  [A loss: 1.020772, acc: 0.031250]\n",
            "505: [D loss: 0.672737, acc: 0.578125]  [A loss: 0.731431, acc: 0.406250]\n",
            "506: [D loss: 0.718639, acc: 0.505859]  [A loss: 1.127771, acc: 0.003906]\n",
            "507: [D loss: 0.669501, acc: 0.609375]  [A loss: 0.665364, acc: 0.585938]\n",
            "508: [D loss: 0.707872, acc: 0.505859]  [A loss: 1.052258, acc: 0.019531]\n",
            "509: [D loss: 0.662174, acc: 0.634766]  [A loss: 0.736179, acc: 0.406250]\n",
            "510: [D loss: 0.692296, acc: 0.517578]  [A loss: 0.999989, acc: 0.027344]\n",
            "511: [D loss: 0.665600, acc: 0.623047]  [A loss: 0.739389, acc: 0.414062]\n",
            "512: [D loss: 0.700840, acc: 0.511719]  [A loss: 0.975598, acc: 0.046875]\n",
            "513: [D loss: 0.668993, acc: 0.605469]  [A loss: 0.794254, acc: 0.300781]\n",
            "514: [D loss: 0.685649, acc: 0.552734]  [A loss: 0.996137, acc: 0.050781]\n",
            "515: [D loss: 0.666173, acc: 0.587891]  [A loss: 0.783803, acc: 0.277344]\n",
            "516: [D loss: 0.691127, acc: 0.527344]  [A loss: 1.028683, acc: 0.039062]\n",
            "517: [D loss: 0.673264, acc: 0.576172]  [A loss: 0.783335, acc: 0.253906]\n",
            "518: [D loss: 0.675924, acc: 0.556641]  [A loss: 1.069855, acc: 0.000000]\n",
            "519: [D loss: 0.664103, acc: 0.603516]  [A loss: 0.733706, acc: 0.398438]\n",
            "520: [D loss: 0.696785, acc: 0.535156]  [A loss: 1.073943, acc: 0.011719]\n",
            "521: [D loss: 0.669261, acc: 0.591797]  [A loss: 0.720859, acc: 0.457031]\n",
            "522: [D loss: 0.716656, acc: 0.523438]  [A loss: 1.072654, acc: 0.023438]\n",
            "523: [D loss: 0.666978, acc: 0.621094]  [A loss: 0.709322, acc: 0.496094]\n",
            "524: [D loss: 0.685368, acc: 0.539062]  [A loss: 0.955584, acc: 0.101562]\n",
            "525: [D loss: 0.691873, acc: 0.544922]  [A loss: 0.805438, acc: 0.250000]\n",
            "526: [D loss: 0.674156, acc: 0.576172]  [A loss: 0.937194, acc: 0.078125]\n",
            "527: [D loss: 0.662070, acc: 0.623047]  [A loss: 0.835100, acc: 0.203125]\n",
            "528: [D loss: 0.670869, acc: 0.566406]  [A loss: 0.932135, acc: 0.101562]\n",
            "529: [D loss: 0.669342, acc: 0.574219]  [A loss: 0.870883, acc: 0.144531]\n",
            "530: [D loss: 0.693382, acc: 0.513672]  [A loss: 1.011045, acc: 0.039062]\n",
            "531: [D loss: 0.659476, acc: 0.611328]  [A loss: 0.857137, acc: 0.207031]\n",
            "532: [D loss: 0.676884, acc: 0.568359]  [A loss: 1.047611, acc: 0.027344]\n",
            "533: [D loss: 0.664535, acc: 0.603516]  [A loss: 0.719633, acc: 0.492188]\n",
            "534: [D loss: 0.720111, acc: 0.492188]  [A loss: 1.195027, acc: 0.003906]\n",
            "535: [D loss: 0.684139, acc: 0.541016]  [A loss: 0.627215, acc: 0.695312]\n",
            "536: [D loss: 0.727457, acc: 0.503906]  [A loss: 1.148463, acc: 0.003906]\n",
            "537: [D loss: 0.677747, acc: 0.599609]  [A loss: 0.656529, acc: 0.609375]\n",
            "538: [D loss: 0.713351, acc: 0.515625]  [A loss: 0.929992, acc: 0.046875]\n",
            "539: [D loss: 0.674823, acc: 0.591797]  [A loss: 0.800613, acc: 0.246094]\n",
            "540: [D loss: 0.685290, acc: 0.564453]  [A loss: 0.936529, acc: 0.082031]\n",
            "541: [D loss: 0.663370, acc: 0.591797]  [A loss: 0.803350, acc: 0.292969]\n",
            "542: [D loss: 0.681793, acc: 0.558594]  [A loss: 0.917143, acc: 0.085938]\n",
            "543: [D loss: 0.668572, acc: 0.603516]  [A loss: 0.878781, acc: 0.128906]\n",
            "544: [D loss: 0.683809, acc: 0.539062]  [A loss: 0.957912, acc: 0.062500]\n",
            "545: [D loss: 0.679228, acc: 0.572266]  [A loss: 0.832683, acc: 0.183594]\n",
            "546: [D loss: 0.671714, acc: 0.570312]  [A loss: 0.994863, acc: 0.046875]\n",
            "547: [D loss: 0.681648, acc: 0.580078]  [A loss: 0.871227, acc: 0.140625]\n",
            "548: [D loss: 0.680814, acc: 0.550781]  [A loss: 0.959602, acc: 0.062500]\n",
            "549: [D loss: 0.670361, acc: 0.597656]  [A loss: 0.794497, acc: 0.289062]\n",
            "550: [D loss: 0.696407, acc: 0.542969]  [A loss: 1.054185, acc: 0.027344]\n",
            "551: [D loss: 0.671822, acc: 0.582031]  [A loss: 0.768210, acc: 0.339844]\n",
            "552: [D loss: 0.691943, acc: 0.560547]  [A loss: 1.115929, acc: 0.011719]\n",
            "553: [D loss: 0.667449, acc: 0.597656]  [A loss: 0.714088, acc: 0.460938]\n",
            "554: [D loss: 0.707384, acc: 0.521484]  [A loss: 1.143924, acc: 0.003906]\n",
            "555: [D loss: 0.674936, acc: 0.578125]  [A loss: 0.662818, acc: 0.613281]\n",
            "556: [D loss: 0.714077, acc: 0.509766]  [A loss: 1.070606, acc: 0.007812]\n",
            "557: [D loss: 0.672212, acc: 0.574219]  [A loss: 0.701129, acc: 0.507812]\n",
            "558: [D loss: 0.686499, acc: 0.542969]  [A loss: 0.967428, acc: 0.031250]\n",
            "559: [D loss: 0.674281, acc: 0.572266]  [A loss: 0.764082, acc: 0.355469]\n",
            "560: [D loss: 0.684325, acc: 0.542969]  [A loss: 0.974708, acc: 0.019531]\n",
            "561: [D loss: 0.677468, acc: 0.539062]  [A loss: 0.846447, acc: 0.171875]\n",
            "562: [D loss: 0.682386, acc: 0.558594]  [A loss: 1.032102, acc: 0.019531]\n",
            "563: [D loss: 0.675841, acc: 0.619141]  [A loss: 0.749873, acc: 0.386719]\n",
            "564: [D loss: 0.687134, acc: 0.544922]  [A loss: 0.984243, acc: 0.046875]\n",
            "565: [D loss: 0.674614, acc: 0.583984]  [A loss: 0.768989, acc: 0.343750]\n",
            "566: [D loss: 0.688332, acc: 0.566406]  [A loss: 1.019355, acc: 0.027344]\n",
            "567: [D loss: 0.666203, acc: 0.585938]  [A loss: 0.769724, acc: 0.328125]\n",
            "568: [D loss: 0.695233, acc: 0.544922]  [A loss: 1.097239, acc: 0.011719]\n",
            "569: [D loss: 0.694057, acc: 0.503906]  [A loss: 0.787225, acc: 0.269531]\n",
            "570: [D loss: 0.678861, acc: 0.568359]  [A loss: 0.921191, acc: 0.066406]\n",
            "571: [D loss: 0.685954, acc: 0.537109]  [A loss: 0.866368, acc: 0.113281]\n",
            "572: [D loss: 0.678834, acc: 0.574219]  [A loss: 0.900529, acc: 0.113281]\n",
            "573: [D loss: 0.680339, acc: 0.568359]  [A loss: 0.900556, acc: 0.167969]\n",
            "574: [D loss: 0.675136, acc: 0.568359]  [A loss: 0.852368, acc: 0.187500]\n",
            "575: [D loss: 0.690868, acc: 0.552734]  [A loss: 0.958756, acc: 0.101562]\n",
            "576: [D loss: 0.673189, acc: 0.611328]  [A loss: 0.833490, acc: 0.226562]\n",
            "577: [D loss: 0.680088, acc: 0.529297]  [A loss: 0.995482, acc: 0.054688]\n",
            "578: [D loss: 0.676263, acc: 0.566406]  [A loss: 0.777204, acc: 0.324219]\n",
            "579: [D loss: 0.696860, acc: 0.537109]  [A loss: 1.104307, acc: 0.007812]\n",
            "580: [D loss: 0.682685, acc: 0.576172]  [A loss: 0.698849, acc: 0.539062]\n",
            "581: [D loss: 0.701558, acc: 0.519531]  [A loss: 1.087636, acc: 0.011719]\n",
            "582: [D loss: 0.672106, acc: 0.587891]  [A loss: 0.733784, acc: 0.363281]\n",
            "583: [D loss: 0.702895, acc: 0.494141]  [A loss: 0.998627, acc: 0.046875]\n",
            "584: [D loss: 0.676094, acc: 0.548828]  [A loss: 0.716376, acc: 0.433594]\n",
            "585: [D loss: 0.719529, acc: 0.501953]  [A loss: 1.066422, acc: 0.019531]\n",
            "586: [D loss: 0.671320, acc: 0.576172]  [A loss: 0.691838, acc: 0.566406]\n",
            "587: [D loss: 0.690542, acc: 0.541016]  [A loss: 0.974471, acc: 0.042969]\n",
            "588: [D loss: 0.668703, acc: 0.595703]  [A loss: 0.771024, acc: 0.304688]\n",
            "589: [D loss: 0.682385, acc: 0.552734]  [A loss: 0.979521, acc: 0.035156]\n",
            "590: [D loss: 0.671417, acc: 0.589844]  [A loss: 0.791670, acc: 0.269531]\n",
            "591: [D loss: 0.678901, acc: 0.587891]  [A loss: 0.911113, acc: 0.128906]\n",
            "592: [D loss: 0.670569, acc: 0.583984]  [A loss: 0.828937, acc: 0.257812]\n",
            "593: [D loss: 0.702325, acc: 0.517578]  [A loss: 0.986579, acc: 0.042969]\n",
            "594: [D loss: 0.660972, acc: 0.640625]  [A loss: 0.817091, acc: 0.234375]\n",
            "595: [D loss: 0.680953, acc: 0.537109]  [A loss: 0.958782, acc: 0.019531]\n",
            "596: [D loss: 0.664831, acc: 0.580078]  [A loss: 0.788855, acc: 0.281250]\n",
            "597: [D loss: 0.684561, acc: 0.560547]  [A loss: 1.055878, acc: 0.027344]\n",
            "598: [D loss: 0.665098, acc: 0.619141]  [A loss: 0.714040, acc: 0.484375]\n",
            "599: [D loss: 0.720606, acc: 0.517578]  [A loss: 1.212666, acc: 0.003906]\n",
            "600: [D loss: 0.677169, acc: 0.548828]  [A loss: 0.656827, acc: 0.636719]\n",
            "601: [D loss: 0.741250, acc: 0.503906]  [A loss: 1.044727, acc: 0.023438]\n",
            "602: [D loss: 0.671001, acc: 0.570312]  [A loss: 0.747172, acc: 0.410156]\n",
            "603: [D loss: 0.689888, acc: 0.542969]  [A loss: 0.886395, acc: 0.113281]\n",
            "604: [D loss: 0.679600, acc: 0.558594]  [A loss: 0.804973, acc: 0.214844]\n",
            "605: [D loss: 0.692408, acc: 0.527344]  [A loss: 0.912825, acc: 0.085938]\n",
            "606: [D loss: 0.676865, acc: 0.570312]  [A loss: 0.804462, acc: 0.250000]\n",
            "607: [D loss: 0.680666, acc: 0.560547]  [A loss: 0.864454, acc: 0.125000]\n",
            "608: [D loss: 0.681627, acc: 0.578125]  [A loss: 0.844140, acc: 0.148438]\n",
            "609: [D loss: 0.690289, acc: 0.542969]  [A loss: 0.867611, acc: 0.183594]\n",
            "610: [D loss: 0.673264, acc: 0.585938]  [A loss: 0.910756, acc: 0.078125]\n",
            "611: [D loss: 0.681668, acc: 0.550781]  [A loss: 0.870065, acc: 0.136719]\n",
            "612: [D loss: 0.668909, acc: 0.582031]  [A loss: 0.964988, acc: 0.070312]\n",
            "613: [D loss: 0.676251, acc: 0.599609]  [A loss: 0.848505, acc: 0.164062]\n",
            "614: [D loss: 0.677440, acc: 0.580078]  [A loss: 0.957134, acc: 0.062500]\n",
            "615: [D loss: 0.685071, acc: 0.556641]  [A loss: 0.817969, acc: 0.238281]\n",
            "616: [D loss: 0.677180, acc: 0.583984]  [A loss: 0.985619, acc: 0.046875]\n",
            "617: [D loss: 0.668565, acc: 0.583984]  [A loss: 0.762530, acc: 0.355469]\n",
            "618: [D loss: 0.710342, acc: 0.513672]  [A loss: 1.139844, acc: 0.003906]\n",
            "619: [D loss: 0.679082, acc: 0.566406]  [A loss: 0.671025, acc: 0.601562]\n",
            "620: [D loss: 0.705748, acc: 0.529297]  [A loss: 1.068188, acc: 0.031250]\n",
            "621: [D loss: 0.667256, acc: 0.609375]  [A loss: 0.703238, acc: 0.472656]\n",
            "622: [D loss: 0.697898, acc: 0.515625]  [A loss: 1.068287, acc: 0.031250]\n",
            "623: [D loss: 0.677239, acc: 0.558594]  [A loss: 0.737161, acc: 0.363281]\n",
            "624: [D loss: 0.705066, acc: 0.519531]  [A loss: 0.962485, acc: 0.089844]\n",
            "625: [D loss: 0.681958, acc: 0.546875]  [A loss: 0.819237, acc: 0.226562]\n",
            "626: [D loss: 0.675420, acc: 0.570312]  [A loss: 0.874211, acc: 0.125000]\n",
            "627: [D loss: 0.681549, acc: 0.546875]  [A loss: 0.875529, acc: 0.152344]\n",
            "628: [D loss: 0.672828, acc: 0.580078]  [A loss: 0.830921, acc: 0.183594]\n",
            "629: [D loss: 0.687365, acc: 0.552734]  [A loss: 0.892054, acc: 0.136719]\n",
            "630: [D loss: 0.684409, acc: 0.541016]  [A loss: 0.887811, acc: 0.117188]\n",
            "631: [D loss: 0.687232, acc: 0.531250]  [A loss: 0.945647, acc: 0.074219]\n",
            "632: [D loss: 0.677602, acc: 0.576172]  [A loss: 0.764555, acc: 0.308594]\n",
            "633: [D loss: 0.703301, acc: 0.521484]  [A loss: 0.981554, acc: 0.035156]\n",
            "634: [D loss: 0.664923, acc: 0.615234]  [A loss: 0.802111, acc: 0.226562]\n",
            "635: [D loss: 0.701161, acc: 0.533203]  [A loss: 0.971944, acc: 0.070312]\n",
            "636: [D loss: 0.671172, acc: 0.611328]  [A loss: 0.799559, acc: 0.250000]\n",
            "637: [D loss: 0.686273, acc: 0.541016]  [A loss: 0.892117, acc: 0.093750]\n",
            "638: [D loss: 0.679391, acc: 0.582031]  [A loss: 0.780887, acc: 0.296875]\n",
            "639: [D loss: 0.689086, acc: 0.539062]  [A loss: 0.992902, acc: 0.019531]\n",
            "640: [D loss: 0.659652, acc: 0.617188]  [A loss: 0.738879, acc: 0.371094]\n",
            "641: [D loss: 0.709666, acc: 0.511719]  [A loss: 1.115555, acc: 0.011719]\n",
            "642: [D loss: 0.680282, acc: 0.568359]  [A loss: 0.720883, acc: 0.449219]\n",
            "643: [D loss: 0.711226, acc: 0.501953]  [A loss: 1.073643, acc: 0.011719]\n",
            "644: [D loss: 0.681919, acc: 0.537109]  [A loss: 0.735316, acc: 0.406250]\n",
            "645: [D loss: 0.694449, acc: 0.521484]  [A loss: 0.986383, acc: 0.039062]\n",
            "646: [D loss: 0.671772, acc: 0.587891]  [A loss: 0.771466, acc: 0.289062]\n",
            "647: [D loss: 0.689567, acc: 0.548828]  [A loss: 0.969879, acc: 0.054688]\n",
            "648: [D loss: 0.670788, acc: 0.605469]  [A loss: 0.773982, acc: 0.312500]\n",
            "649: [D loss: 0.672251, acc: 0.568359]  [A loss: 1.030967, acc: 0.054688]\n",
            "650: [D loss: 0.675003, acc: 0.574219]  [A loss: 0.801552, acc: 0.281250]\n",
            "651: [D loss: 0.682777, acc: 0.562500]  [A loss: 0.920953, acc: 0.105469]\n",
            "652: [D loss: 0.679212, acc: 0.582031]  [A loss: 0.875620, acc: 0.156250]\n",
            "653: [D loss: 0.668347, acc: 0.591797]  [A loss: 0.826441, acc: 0.199219]\n",
            "654: [D loss: 0.698663, acc: 0.521484]  [A loss: 1.020014, acc: 0.027344]\n",
            "655: [D loss: 0.669454, acc: 0.601562]  [A loss: 0.797339, acc: 0.281250]\n",
            "656: [D loss: 0.688976, acc: 0.548828]  [A loss: 0.964680, acc: 0.050781]\n",
            "657: [D loss: 0.671038, acc: 0.589844]  [A loss: 0.772464, acc: 0.320312]\n",
            "658: [D loss: 0.684466, acc: 0.542969]  [A loss: 0.949450, acc: 0.085938]\n",
            "659: [D loss: 0.662302, acc: 0.619141]  [A loss: 0.763068, acc: 0.367188]\n",
            "660: [D loss: 0.693190, acc: 0.539062]  [A loss: 0.962991, acc: 0.050781]\n",
            "661: [D loss: 0.660684, acc: 0.601562]  [A loss: 0.755992, acc: 0.343750]\n",
            "662: [D loss: 0.695772, acc: 0.541016]  [A loss: 1.020923, acc: 0.027344]\n",
            "663: [D loss: 0.673814, acc: 0.546875]  [A loss: 0.728631, acc: 0.421875]\n",
            "664: [D loss: 0.704275, acc: 0.529297]  [A loss: 1.078662, acc: 0.007812]\n",
            "665: [D loss: 0.675128, acc: 0.544922]  [A loss: 0.694923, acc: 0.511719]\n",
            "666: [D loss: 0.692584, acc: 0.527344]  [A loss: 1.000112, acc: 0.050781]\n",
            "667: [D loss: 0.667889, acc: 0.593750]  [A loss: 0.699200, acc: 0.488281]\n",
            "668: [D loss: 0.717124, acc: 0.533203]  [A loss: 1.110686, acc: 0.003906]\n",
            "669: [D loss: 0.679466, acc: 0.556641]  [A loss: 0.711032, acc: 0.445312]\n",
            "670: [D loss: 0.712278, acc: 0.515625]  [A loss: 0.960319, acc: 0.105469]\n",
            "671: [D loss: 0.679700, acc: 0.578125]  [A loss: 0.763133, acc: 0.324219]\n",
            "672: [D loss: 0.680348, acc: 0.529297]  [A loss: 0.935505, acc: 0.066406]\n",
            "673: [D loss: 0.677209, acc: 0.582031]  [A loss: 0.794854, acc: 0.257812]\n",
            "674: [D loss: 0.694477, acc: 0.533203]  [A loss: 0.907441, acc: 0.097656]\n",
            "675: [D loss: 0.669398, acc: 0.605469]  [A loss: 0.814698, acc: 0.207031]\n",
            "676: [D loss: 0.692811, acc: 0.537109]  [A loss: 0.877474, acc: 0.121094]\n",
            "677: [D loss: 0.664350, acc: 0.572266]  [A loss: 0.872799, acc: 0.152344]\n",
            "678: [D loss: 0.676588, acc: 0.560547]  [A loss: 0.850590, acc: 0.167969]\n",
            "679: [D loss: 0.693037, acc: 0.542969]  [A loss: 0.937050, acc: 0.066406]\n",
            "680: [D loss: 0.678364, acc: 0.570312]  [A loss: 0.839426, acc: 0.187500]\n",
            "681: [D loss: 0.684963, acc: 0.566406]  [A loss: 0.916504, acc: 0.113281]\n",
            "682: [D loss: 0.673770, acc: 0.580078]  [A loss: 0.849912, acc: 0.160156]\n",
            "683: [D loss: 0.668640, acc: 0.599609]  [A loss: 0.932708, acc: 0.101562]\n",
            "684: [D loss: 0.675332, acc: 0.574219]  [A loss: 0.838929, acc: 0.187500]\n",
            "685: [D loss: 0.668744, acc: 0.558594]  [A loss: 0.894684, acc: 0.117188]\n",
            "686: [D loss: 0.665299, acc: 0.607422]  [A loss: 0.932511, acc: 0.093750]\n",
            "687: [D loss: 0.682130, acc: 0.572266]  [A loss: 0.964467, acc: 0.070312]\n",
            "688: [D loss: 0.660661, acc: 0.585938]  [A loss: 0.821433, acc: 0.261719]\n",
            "689: [D loss: 0.685335, acc: 0.548828]  [A loss: 1.076540, acc: 0.015625]\n",
            "690: [D loss: 0.671852, acc: 0.591797]  [A loss: 0.661725, acc: 0.605469]\n",
            "691: [D loss: 0.718201, acc: 0.525391]  [A loss: 1.157404, acc: 0.000000]\n",
            "692: [D loss: 0.680738, acc: 0.558594]  [A loss: 0.639307, acc: 0.628906]\n",
            "693: [D loss: 0.724140, acc: 0.507812]  [A loss: 1.045053, acc: 0.027344]\n",
            "694: [D loss: 0.655932, acc: 0.611328]  [A loss: 0.721840, acc: 0.417969]\n",
            "695: [D loss: 0.699135, acc: 0.535156]  [A loss: 0.915376, acc: 0.082031]\n",
            "696: [D loss: 0.688736, acc: 0.539062]  [A loss: 0.747692, acc: 0.355469]\n",
            "697: [D loss: 0.696047, acc: 0.531250]  [A loss: 0.883277, acc: 0.140625]\n",
            "698: [D loss: 0.676963, acc: 0.558594]  [A loss: 0.837489, acc: 0.160156]\n",
            "699: [D loss: 0.681409, acc: 0.566406]  [A loss: 0.870684, acc: 0.148438]\n",
            "700: [D loss: 0.677805, acc: 0.564453]  [A loss: 0.898957, acc: 0.089844]\n",
            "701: [D loss: 0.675341, acc: 0.570312]  [A loss: 0.901691, acc: 0.105469]\n",
            "702: [D loss: 0.680823, acc: 0.558594]  [A loss: 0.818456, acc: 0.210938]\n",
            "703: [D loss: 0.684387, acc: 0.572266]  [A loss: 0.958220, acc: 0.031250]\n",
            "704: [D loss: 0.668635, acc: 0.611328]  [A loss: 0.818856, acc: 0.246094]\n",
            "705: [D loss: 0.701086, acc: 0.539062]  [A loss: 1.042607, acc: 0.015625]\n",
            "706: [D loss: 0.659160, acc: 0.632812]  [A loss: 0.686236, acc: 0.554688]\n",
            "707: [D loss: 0.691076, acc: 0.523438]  [A loss: 1.030212, acc: 0.050781]\n",
            "708: [D loss: 0.674940, acc: 0.560547]  [A loss: 0.763837, acc: 0.312500]\n",
            "709: [D loss: 0.708188, acc: 0.517578]  [A loss: 1.053415, acc: 0.042969]\n",
            "710: [D loss: 0.679003, acc: 0.556641]  [A loss: 0.748257, acc: 0.414062]\n",
            "711: [D loss: 0.702515, acc: 0.529297]  [A loss: 0.982432, acc: 0.062500]\n",
            "712: [D loss: 0.672416, acc: 0.595703]  [A loss: 0.755338, acc: 0.394531]\n",
            "713: [D loss: 0.690940, acc: 0.562500]  [A loss: 0.957600, acc: 0.062500]\n",
            "714: [D loss: 0.680295, acc: 0.589844]  [A loss: 0.785621, acc: 0.277344]\n",
            "715: [D loss: 0.683615, acc: 0.533203]  [A loss: 0.913102, acc: 0.113281]\n",
            "716: [D loss: 0.684925, acc: 0.560547]  [A loss: 0.862786, acc: 0.167969]\n",
            "717: [D loss: 0.684316, acc: 0.552734]  [A loss: 0.920463, acc: 0.074219]\n",
            "718: [D loss: 0.675653, acc: 0.580078]  [A loss: 0.845999, acc: 0.179688]\n",
            "719: [D loss: 0.674766, acc: 0.568359]  [A loss: 0.879434, acc: 0.140625]\n",
            "720: [D loss: 0.676094, acc: 0.566406]  [A loss: 0.909480, acc: 0.109375]\n",
            "721: [D loss: 0.676922, acc: 0.562500]  [A loss: 0.883973, acc: 0.136719]\n",
            "722: [D loss: 0.690441, acc: 0.550781]  [A loss: 0.985666, acc: 0.066406]\n",
            "723: [D loss: 0.679096, acc: 0.556641]  [A loss: 0.748781, acc: 0.347656]\n",
            "724: [D loss: 0.695555, acc: 0.525391]  [A loss: 1.121245, acc: 0.007812]\n",
            "725: [D loss: 0.675058, acc: 0.591797]  [A loss: 0.678465, acc: 0.566406]\n",
            "726: [D loss: 0.697921, acc: 0.529297]  [A loss: 0.961146, acc: 0.078125]\n",
            "727: [D loss: 0.683214, acc: 0.531250]  [A loss: 0.763888, acc: 0.304688]\n",
            "728: [D loss: 0.690773, acc: 0.548828]  [A loss: 0.970286, acc: 0.050781]\n",
            "729: [D loss: 0.656654, acc: 0.607422]  [A loss: 0.822580, acc: 0.203125]\n",
            "730: [D loss: 0.686472, acc: 0.542969]  [A loss: 0.948093, acc: 0.074219]\n",
            "731: [D loss: 0.679655, acc: 0.585938]  [A loss: 0.803570, acc: 0.281250]\n",
            "732: [D loss: 0.687613, acc: 0.552734]  [A loss: 0.901324, acc: 0.121094]\n",
            "733: [D loss: 0.687039, acc: 0.566406]  [A loss: 0.837322, acc: 0.187500]\n",
            "734: [D loss: 0.678958, acc: 0.556641]  [A loss: 0.882463, acc: 0.144531]\n",
            "735: [D loss: 0.685316, acc: 0.562500]  [A loss: 0.926464, acc: 0.101562]\n",
            "736: [D loss: 0.669894, acc: 0.562500]  [A loss: 0.875563, acc: 0.148438]\n",
            "737: [D loss: 0.683967, acc: 0.558594]  [A loss: 0.883840, acc: 0.132812]\n",
            "738: [D loss: 0.662480, acc: 0.605469]  [A loss: 0.821806, acc: 0.218750]\n",
            "739: [D loss: 0.696815, acc: 0.527344]  [A loss: 1.087073, acc: 0.011719]\n",
            "740: [D loss: 0.685008, acc: 0.544922]  [A loss: 0.704544, acc: 0.511719]\n",
            "741: [D loss: 0.715450, acc: 0.519531]  [A loss: 1.097857, acc: 0.019531]\n",
            "742: [D loss: 0.677036, acc: 0.572266]  [A loss: 0.648273, acc: 0.640625]\n",
            "743: [D loss: 0.725963, acc: 0.505859]  [A loss: 0.992555, acc: 0.023438]\n",
            "744: [D loss: 0.674737, acc: 0.583984]  [A loss: 0.744862, acc: 0.371094]\n",
            "745: [D loss: 0.689116, acc: 0.548828]  [A loss: 0.880348, acc: 0.128906]\n",
            "746: [D loss: 0.679438, acc: 0.558594]  [A loss: 0.780556, acc: 0.277344]\n",
            "747: [D loss: 0.700440, acc: 0.544922]  [A loss: 0.874578, acc: 0.117188]\n",
            "748: [D loss: 0.686154, acc: 0.542969]  [A loss: 0.817193, acc: 0.183594]\n",
            "749: [D loss: 0.695102, acc: 0.546875]  [A loss: 0.874539, acc: 0.082031]\n",
            "750: [D loss: 0.673760, acc: 0.574219]  [A loss: 0.809535, acc: 0.261719]\n",
            "751: [D loss: 0.706244, acc: 0.521484]  [A loss: 1.001458, acc: 0.027344]\n",
            "752: [D loss: 0.681686, acc: 0.542969]  [A loss: 0.745281, acc: 0.367188]\n",
            "753: [D loss: 0.697472, acc: 0.533203]  [A loss: 1.003700, acc: 0.011719]\n",
            "754: [D loss: 0.675549, acc: 0.609375]  [A loss: 0.706424, acc: 0.472656]\n",
            "755: [D loss: 0.692434, acc: 0.507812]  [A loss: 0.964257, acc: 0.046875]\n",
            "756: [D loss: 0.680211, acc: 0.560547]  [A loss: 0.763338, acc: 0.347656]\n",
            "757: [D loss: 0.694818, acc: 0.541016]  [A loss: 0.964593, acc: 0.066406]\n",
            "758: [D loss: 0.671493, acc: 0.597656]  [A loss: 0.727412, acc: 0.433594]\n",
            "759: [D loss: 0.695533, acc: 0.541016]  [A loss: 1.004057, acc: 0.039062]\n",
            "760: [D loss: 0.675443, acc: 0.583984]  [A loss: 0.736250, acc: 0.382812]\n",
            "761: [D loss: 0.693075, acc: 0.509766]  [A loss: 0.911304, acc: 0.066406]\n",
            "762: [D loss: 0.673335, acc: 0.572266]  [A loss: 0.819540, acc: 0.230469]\n",
            "763: [D loss: 0.672595, acc: 0.568359]  [A loss: 0.868188, acc: 0.152344]\n",
            "764: [D loss: 0.670821, acc: 0.595703]  [A loss: 0.887460, acc: 0.105469]\n",
            "765: [D loss: 0.660522, acc: 0.611328]  [A loss: 0.866179, acc: 0.171875]\n",
            "766: [D loss: 0.680851, acc: 0.564453]  [A loss: 0.976767, acc: 0.039062]\n",
            "767: [D loss: 0.674878, acc: 0.582031]  [A loss: 0.869707, acc: 0.152344]\n",
            "768: [D loss: 0.674856, acc: 0.554688]  [A loss: 0.869928, acc: 0.117188]\n",
            "769: [D loss: 0.690353, acc: 0.519531]  [A loss: 0.862777, acc: 0.191406]\n",
            "770: [D loss: 0.675812, acc: 0.585938]  [A loss: 0.878223, acc: 0.148438]\n",
            "771: [D loss: 0.674267, acc: 0.603516]  [A loss: 0.893051, acc: 0.097656]\n",
            "772: [D loss: 0.675908, acc: 0.572266]  [A loss: 0.928901, acc: 0.070312]\n",
            "773: [D loss: 0.675394, acc: 0.570312]  [A loss: 0.885449, acc: 0.140625]\n",
            "774: [D loss: 0.693326, acc: 0.519531]  [A loss: 0.946721, acc: 0.070312]\n",
            "775: [D loss: 0.678894, acc: 0.564453]  [A loss: 0.743893, acc: 0.371094]\n",
            "776: [D loss: 0.715984, acc: 0.500000]  [A loss: 1.088228, acc: 0.023438]\n",
            "777: [D loss: 0.671943, acc: 0.621094]  [A loss: 0.717604, acc: 0.460938]\n",
            "778: [D loss: 0.717752, acc: 0.517578]  [A loss: 1.122459, acc: 0.011719]\n",
            "779: [D loss: 0.687007, acc: 0.519531]  [A loss: 0.662241, acc: 0.632812]\n",
            "780: [D loss: 0.716265, acc: 0.523438]  [A loss: 0.969603, acc: 0.054688]\n",
            "781: [D loss: 0.673114, acc: 0.580078]  [A loss: 0.730413, acc: 0.437500]\n",
            "782: [D loss: 0.689287, acc: 0.525391]  [A loss: 0.892730, acc: 0.097656]\n",
            "783: [D loss: 0.677696, acc: 0.572266]  [A loss: 0.826766, acc: 0.234375]\n",
            "784: [D loss: 0.681941, acc: 0.556641]  [A loss: 0.836269, acc: 0.179688]\n",
            "785: [D loss: 0.690463, acc: 0.537109]  [A loss: 0.887152, acc: 0.101562]\n",
            "786: [D loss: 0.680137, acc: 0.566406]  [A loss: 0.773471, acc: 0.296875]\n",
            "787: [D loss: 0.686878, acc: 0.556641]  [A loss: 0.898813, acc: 0.128906]\n",
            "788: [D loss: 0.679845, acc: 0.576172]  [A loss: 0.789347, acc: 0.253906]\n",
            "789: [D loss: 0.683818, acc: 0.548828]  [A loss: 0.892148, acc: 0.089844]\n",
            "790: [D loss: 0.669604, acc: 0.607422]  [A loss: 0.775052, acc: 0.347656]\n",
            "791: [D loss: 0.701936, acc: 0.513672]  [A loss: 0.917630, acc: 0.093750]\n",
            "792: [D loss: 0.681213, acc: 0.568359]  [A loss: 0.800372, acc: 0.277344]\n",
            "793: [D loss: 0.717000, acc: 0.529297]  [A loss: 1.015103, acc: 0.027344]\n",
            "794: [D loss: 0.685009, acc: 0.574219]  [A loss: 0.671611, acc: 0.601562]\n",
            "795: [D loss: 0.688284, acc: 0.541016]  [A loss: 0.935927, acc: 0.082031]\n",
            "796: [D loss: 0.669266, acc: 0.609375]  [A loss: 0.730625, acc: 0.417969]\n",
            "797: [D loss: 0.702449, acc: 0.521484]  [A loss: 0.955055, acc: 0.046875]\n",
            "798: [D loss: 0.680175, acc: 0.570312]  [A loss: 0.740646, acc: 0.378906]\n",
            "799: [D loss: 0.689245, acc: 0.550781]  [A loss: 0.904925, acc: 0.074219]\n",
            "800: [D loss: 0.677363, acc: 0.605469]  [A loss: 0.753852, acc: 0.332031]\n",
            "801: [D loss: 0.680071, acc: 0.562500]  [A loss: 0.903419, acc: 0.128906]\n",
            "802: [D loss: 0.663238, acc: 0.601562]  [A loss: 0.854480, acc: 0.179688]\n",
            "803: [D loss: 0.688079, acc: 0.529297]  [A loss: 0.887934, acc: 0.160156]\n",
            "804: [D loss: 0.674282, acc: 0.558594]  [A loss: 0.893075, acc: 0.066406]\n",
            "805: [D loss: 0.681733, acc: 0.566406]  [A loss: 0.799836, acc: 0.218750]\n",
            "806: [D loss: 0.682861, acc: 0.587891]  [A loss: 0.923350, acc: 0.070312]\n",
            "807: [D loss: 0.672467, acc: 0.582031]  [A loss: 0.808687, acc: 0.289062]\n",
            "808: [D loss: 0.694935, acc: 0.511719]  [A loss: 0.979498, acc: 0.035156]\n",
            "809: [D loss: 0.664417, acc: 0.605469]  [A loss: 0.756293, acc: 0.371094]\n",
            "810: [D loss: 0.696890, acc: 0.523438]  [A loss: 1.046026, acc: 0.031250]\n",
            "811: [D loss: 0.670199, acc: 0.607422]  [A loss: 0.665041, acc: 0.585938]\n",
            "812: [D loss: 0.709461, acc: 0.519531]  [A loss: 1.007149, acc: 0.023438]\n",
            "813: [D loss: 0.672891, acc: 0.599609]  [A loss: 0.731704, acc: 0.445312]\n",
            "814: [D loss: 0.694926, acc: 0.519531]  [A loss: 0.924550, acc: 0.097656]\n",
            "815: [D loss: 0.672839, acc: 0.572266]  [A loss: 0.808390, acc: 0.242188]\n",
            "816: [D loss: 0.698047, acc: 0.525391]  [A loss: 0.964853, acc: 0.039062]\n",
            "817: [D loss: 0.669146, acc: 0.593750]  [A loss: 0.711093, acc: 0.468750]\n",
            "818: [D loss: 0.690570, acc: 0.556641]  [A loss: 0.982748, acc: 0.035156]\n",
            "819: [D loss: 0.676280, acc: 0.583984]  [A loss: 0.772837, acc: 0.304688]\n",
            "820: [D loss: 0.690738, acc: 0.560547]  [A loss: 0.929270, acc: 0.062500]\n",
            "821: [D loss: 0.683901, acc: 0.542969]  [A loss: 0.770448, acc: 0.312500]\n",
            "822: [D loss: 0.697592, acc: 0.509766]  [A loss: 0.891785, acc: 0.113281]\n",
            "823: [D loss: 0.667246, acc: 0.582031]  [A loss: 0.843798, acc: 0.164062]\n",
            "824: [D loss: 0.688985, acc: 0.546875]  [A loss: 0.891663, acc: 0.148438]\n",
            "825: [D loss: 0.666961, acc: 0.609375]  [A loss: 0.835638, acc: 0.199219]\n",
            "826: [D loss: 0.692934, acc: 0.535156]  [A loss: 0.850617, acc: 0.152344]\n",
            "827: [D loss: 0.686576, acc: 0.539062]  [A loss: 0.901214, acc: 0.109375]\n",
            "828: [D loss: 0.674411, acc: 0.560547]  [A loss: 0.799701, acc: 0.289062]\n",
            "829: [D loss: 0.692050, acc: 0.562500]  [A loss: 0.962924, acc: 0.042969]\n",
            "830: [D loss: 0.683051, acc: 0.548828]  [A loss: 0.751198, acc: 0.382812]\n",
            "831: [D loss: 0.706255, acc: 0.521484]  [A loss: 1.041376, acc: 0.062500]\n",
            "832: [D loss: 0.680040, acc: 0.560547]  [A loss: 0.719052, acc: 0.472656]\n",
            "833: [D loss: 0.700072, acc: 0.533203]  [A loss: 0.934443, acc: 0.082031]\n",
            "834: [D loss: 0.680407, acc: 0.574219]  [A loss: 0.754374, acc: 0.410156]\n",
            "835: [D loss: 0.693716, acc: 0.527344]  [A loss: 0.943724, acc: 0.066406]\n",
            "836: [D loss: 0.669792, acc: 0.572266]  [A loss: 0.786443, acc: 0.312500]\n",
            "837: [D loss: 0.695255, acc: 0.541016]  [A loss: 0.964980, acc: 0.042969]\n",
            "838: [D loss: 0.676256, acc: 0.554688]  [A loss: 0.716373, acc: 0.410156]\n",
            "839: [D loss: 0.696437, acc: 0.529297]  [A loss: 0.956255, acc: 0.074219]\n",
            "840: [D loss: 0.663036, acc: 0.615234]  [A loss: 0.786845, acc: 0.300781]\n",
            "841: [D loss: 0.688984, acc: 0.568359]  [A loss: 0.928819, acc: 0.078125]\n",
            "842: [D loss: 0.675301, acc: 0.607422]  [A loss: 0.803134, acc: 0.273438]\n",
            "843: [D loss: 0.686903, acc: 0.541016]  [A loss: 0.991370, acc: 0.070312]\n",
            "844: [D loss: 0.675775, acc: 0.578125]  [A loss: 0.761756, acc: 0.367188]\n",
            "845: [D loss: 0.677376, acc: 0.552734]  [A loss: 0.906653, acc: 0.125000]\n",
            "846: [D loss: 0.673090, acc: 0.593750]  [A loss: 0.782352, acc: 0.324219]\n",
            "847: [D loss: 0.691337, acc: 0.537109]  [A loss: 0.956803, acc: 0.062500]\n",
            "848: [D loss: 0.679608, acc: 0.556641]  [A loss: 0.811015, acc: 0.253906]\n",
            "849: [D loss: 0.691954, acc: 0.546875]  [A loss: 0.921599, acc: 0.093750]\n",
            "850: [D loss: 0.660084, acc: 0.623047]  [A loss: 0.797069, acc: 0.269531]\n",
            "851: [D loss: 0.683056, acc: 0.572266]  [A loss: 0.971466, acc: 0.074219]\n",
            "852: [D loss: 0.682382, acc: 0.587891]  [A loss: 0.803074, acc: 0.257812]\n",
            "853: [D loss: 0.710351, acc: 0.515625]  [A loss: 0.976584, acc: 0.066406]\n",
            "854: [D loss: 0.661627, acc: 0.623047]  [A loss: 0.768924, acc: 0.355469]\n",
            "855: [D loss: 0.687216, acc: 0.564453]  [A loss: 0.932772, acc: 0.066406]\n",
            "856: [D loss: 0.684079, acc: 0.562500]  [A loss: 0.795932, acc: 0.269531]\n",
            "857: [D loss: 0.698409, acc: 0.527344]  [A loss: 0.960066, acc: 0.085938]\n",
            "858: [D loss: 0.682832, acc: 0.591797]  [A loss: 0.828023, acc: 0.207031]\n",
            "859: [D loss: 0.672542, acc: 0.568359]  [A loss: 0.922765, acc: 0.105469]\n",
            "860: [D loss: 0.684710, acc: 0.560547]  [A loss: 0.832447, acc: 0.230469]\n",
            "861: [D loss: 0.678346, acc: 0.560547]  [A loss: 0.861875, acc: 0.152344]\n",
            "862: [D loss: 0.676911, acc: 0.574219]  [A loss: 0.853384, acc: 0.171875]\n",
            "863: [D loss: 0.680367, acc: 0.523438]  [A loss: 0.868596, acc: 0.218750]\n",
            "864: [D loss: 0.677567, acc: 0.580078]  [A loss: 0.873902, acc: 0.128906]\n",
            "865: [D loss: 0.682264, acc: 0.568359]  [A loss: 0.878122, acc: 0.132812]\n",
            "866: [D loss: 0.668066, acc: 0.617188]  [A loss: 0.819133, acc: 0.222656]\n",
            "867: [D loss: 0.682168, acc: 0.566406]  [A loss: 1.051780, acc: 0.023438]\n",
            "868: [D loss: 0.682822, acc: 0.552734]  [A loss: 0.648398, acc: 0.640625]\n",
            "869: [D loss: 0.714424, acc: 0.517578]  [A loss: 1.051662, acc: 0.019531]\n",
            "870: [D loss: 0.674675, acc: 0.576172]  [A loss: 0.747502, acc: 0.339844]\n",
            "871: [D loss: 0.697035, acc: 0.525391]  [A loss: 0.930661, acc: 0.078125]\n",
            "872: [D loss: 0.670783, acc: 0.589844]  [A loss: 0.802361, acc: 0.277344]\n",
            "873: [D loss: 0.685077, acc: 0.556641]  [A loss: 0.954964, acc: 0.066406]\n",
            "874: [D loss: 0.686539, acc: 0.570312]  [A loss: 0.769884, acc: 0.308594]\n",
            "875: [D loss: 0.690783, acc: 0.550781]  [A loss: 0.887863, acc: 0.117188]\n",
            "876: [D loss: 0.682978, acc: 0.550781]  [A loss: 0.841048, acc: 0.175781]\n",
            "877: [D loss: 0.690280, acc: 0.560547]  [A loss: 0.914615, acc: 0.113281]\n",
            "878: [D loss: 0.674253, acc: 0.591797]  [A loss: 0.811289, acc: 0.210938]\n",
            "879: [D loss: 0.686497, acc: 0.558594]  [A loss: 0.930525, acc: 0.066406]\n",
            "880: [D loss: 0.681726, acc: 0.554688]  [A loss: 0.729998, acc: 0.390625]\n",
            "881: [D loss: 0.689703, acc: 0.541016]  [A loss: 0.953718, acc: 0.031250]\n",
            "882: [D loss: 0.673493, acc: 0.578125]  [A loss: 0.763126, acc: 0.320312]\n",
            "883: [D loss: 0.682018, acc: 0.539062]  [A loss: 0.977295, acc: 0.074219]\n",
            "884: [D loss: 0.668909, acc: 0.621094]  [A loss: 0.716334, acc: 0.445312]\n",
            "885: [D loss: 0.693130, acc: 0.529297]  [A loss: 1.026442, acc: 0.050781]\n",
            "886: [D loss: 0.684846, acc: 0.542969]  [A loss: 0.707447, acc: 0.457031]\n",
            "887: [D loss: 0.692647, acc: 0.539062]  [A loss: 0.988737, acc: 0.074219]\n",
            "888: [D loss: 0.677206, acc: 0.568359]  [A loss: 0.745957, acc: 0.398438]\n",
            "889: [D loss: 0.691294, acc: 0.541016]  [A loss: 0.895009, acc: 0.125000]\n",
            "890: [D loss: 0.672433, acc: 0.605469]  [A loss: 0.787264, acc: 0.300781]\n",
            "891: [D loss: 0.686254, acc: 0.535156]  [A loss: 0.874924, acc: 0.117188]\n",
            "892: [D loss: 0.679728, acc: 0.578125]  [A loss: 0.846467, acc: 0.183594]\n",
            "893: [D loss: 0.675537, acc: 0.583984]  [A loss: 0.924880, acc: 0.136719]\n",
            "894: [D loss: 0.678069, acc: 0.583984]  [A loss: 0.767996, acc: 0.328125]\n",
            "895: [D loss: 0.698494, acc: 0.537109]  [A loss: 0.928246, acc: 0.113281]\n",
            "896: [D loss: 0.677539, acc: 0.574219]  [A loss: 0.755528, acc: 0.351562]\n",
            "897: [D loss: 0.702946, acc: 0.501953]  [A loss: 0.963454, acc: 0.062500]\n",
            "898: [D loss: 0.678788, acc: 0.556641]  [A loss: 0.755110, acc: 0.367188]\n",
            "899: [D loss: 0.691938, acc: 0.556641]  [A loss: 0.954900, acc: 0.039062]\n",
            "900: [D loss: 0.670440, acc: 0.599609]  [A loss: 0.786699, acc: 0.312500]\n",
            "901: [D loss: 0.699863, acc: 0.529297]  [A loss: 0.927323, acc: 0.062500]\n",
            "902: [D loss: 0.681962, acc: 0.568359]  [A loss: 0.777558, acc: 0.296875]\n",
            "903: [D loss: 0.693812, acc: 0.537109]  [A loss: 0.926038, acc: 0.070312]\n",
            "904: [D loss: 0.680161, acc: 0.576172]  [A loss: 0.752665, acc: 0.398438]\n",
            "905: [D loss: 0.693084, acc: 0.558594]  [A loss: 0.953840, acc: 0.082031]\n",
            "906: [D loss: 0.675997, acc: 0.585938]  [A loss: 0.749581, acc: 0.359375]\n",
            "907: [D loss: 0.687763, acc: 0.556641]  [A loss: 0.937211, acc: 0.074219]\n",
            "908: [D loss: 0.677549, acc: 0.582031]  [A loss: 0.817894, acc: 0.230469]\n",
            "909: [D loss: 0.681399, acc: 0.560547]  [A loss: 0.861292, acc: 0.187500]\n",
            "910: [D loss: 0.683454, acc: 0.562500]  [A loss: 0.826855, acc: 0.195312]\n",
            "911: [D loss: 0.679200, acc: 0.564453]  [A loss: 0.913757, acc: 0.097656]\n",
            "912: [D loss: 0.680313, acc: 0.566406]  [A loss: 0.775316, acc: 0.273438]\n",
            "913: [D loss: 0.692893, acc: 0.537109]  [A loss: 0.929443, acc: 0.085938]\n",
            "914: [D loss: 0.670714, acc: 0.597656]  [A loss: 0.787256, acc: 0.265625]\n",
            "915: [D loss: 0.681367, acc: 0.546875]  [A loss: 0.931811, acc: 0.082031]\n",
            "916: [D loss: 0.676172, acc: 0.585938]  [A loss: 0.789622, acc: 0.265625]\n",
            "917: [D loss: 0.693964, acc: 0.548828]  [A loss: 0.979644, acc: 0.042969]\n",
            "918: [D loss: 0.678726, acc: 0.582031]  [A loss: 0.700757, acc: 0.542969]\n",
            "919: [D loss: 0.703759, acc: 0.521484]  [A loss: 1.057231, acc: 0.015625]\n",
            "920: [D loss: 0.679366, acc: 0.562500]  [A loss: 0.698436, acc: 0.535156]\n",
            "921: [D loss: 0.699720, acc: 0.527344]  [A loss: 0.962223, acc: 0.062500]\n",
            "922: [D loss: 0.662775, acc: 0.591797]  [A loss: 0.747179, acc: 0.378906]\n",
            "923: [D loss: 0.703934, acc: 0.517578]  [A loss: 0.944200, acc: 0.082031]\n",
            "924: [D loss: 0.683193, acc: 0.566406]  [A loss: 0.794181, acc: 0.242188]\n",
            "925: [D loss: 0.681837, acc: 0.554688]  [A loss: 0.861307, acc: 0.167969]\n",
            "926: [D loss: 0.671496, acc: 0.603516]  [A loss: 0.826618, acc: 0.210938]\n",
            "927: [D loss: 0.688152, acc: 0.554688]  [A loss: 0.859834, acc: 0.167969]\n",
            "928: [D loss: 0.675815, acc: 0.585938]  [A loss: 0.828572, acc: 0.210938]\n",
            "929: [D loss: 0.688446, acc: 0.544922]  [A loss: 0.915908, acc: 0.109375]\n",
            "930: [D loss: 0.666738, acc: 0.589844]  [A loss: 0.768187, acc: 0.320312]\n",
            "931: [D loss: 0.683927, acc: 0.560547]  [A loss: 0.933724, acc: 0.070312]\n",
            "932: [D loss: 0.665684, acc: 0.601562]  [A loss: 0.793447, acc: 0.261719]\n",
            "933: [D loss: 0.679145, acc: 0.546875]  [A loss: 0.958440, acc: 0.078125]\n",
            "934: [D loss: 0.670482, acc: 0.595703]  [A loss: 0.722629, acc: 0.445312]\n",
            "935: [D loss: 0.691532, acc: 0.527344]  [A loss: 0.938373, acc: 0.101562]\n",
            "936: [D loss: 0.675601, acc: 0.566406]  [A loss: 0.781769, acc: 0.320312]\n",
            "937: [D loss: 0.708551, acc: 0.519531]  [A loss: 1.067010, acc: 0.011719]\n",
            "938: [D loss: 0.672936, acc: 0.572266]  [A loss: 0.671591, acc: 0.554688]\n",
            "939: [D loss: 0.704018, acc: 0.519531]  [A loss: 0.970804, acc: 0.058594]\n",
            "940: [D loss: 0.685697, acc: 0.558594]  [A loss: 0.766924, acc: 0.300781]\n",
            "941: [D loss: 0.702582, acc: 0.511719]  [A loss: 0.938276, acc: 0.082031]\n",
            "942: [D loss: 0.669229, acc: 0.595703]  [A loss: 0.753052, acc: 0.359375]\n",
            "943: [D loss: 0.688417, acc: 0.550781]  [A loss: 0.891635, acc: 0.093750]\n",
            "944: [D loss: 0.677595, acc: 0.580078]  [A loss: 0.810036, acc: 0.265625]\n",
            "945: [D loss: 0.682318, acc: 0.546875]  [A loss: 0.897529, acc: 0.085938]\n",
            "946: [D loss: 0.669689, acc: 0.603516]  [A loss: 0.822098, acc: 0.261719]\n",
            "947: [D loss: 0.682532, acc: 0.562500]  [A loss: 0.896100, acc: 0.136719]\n",
            "948: [D loss: 0.676570, acc: 0.548828]  [A loss: 0.800383, acc: 0.242188]\n",
            "949: [D loss: 0.685162, acc: 0.550781]  [A loss: 0.927962, acc: 0.101562]\n",
            "950: [D loss: 0.679139, acc: 0.564453]  [A loss: 0.784248, acc: 0.296875]\n",
            "951: [D loss: 0.689958, acc: 0.529297]  [A loss: 0.976291, acc: 0.039062]\n",
            "952: [D loss: 0.676920, acc: 0.578125]  [A loss: 0.822633, acc: 0.257812]\n",
            "953: [D loss: 0.678846, acc: 0.564453]  [A loss: 0.887585, acc: 0.156250]\n",
            "954: [D loss: 0.676135, acc: 0.554688]  [A loss: 0.852810, acc: 0.175781]\n",
            "955: [D loss: 0.681786, acc: 0.570312]  [A loss: 0.961046, acc: 0.070312]\n",
            "956: [D loss: 0.673269, acc: 0.595703]  [A loss: 0.783048, acc: 0.324219]\n",
            "957: [D loss: 0.693437, acc: 0.533203]  [A loss: 0.992169, acc: 0.046875]\n",
            "958: [D loss: 0.674361, acc: 0.560547]  [A loss: 0.712832, acc: 0.464844]\n",
            "959: [D loss: 0.732350, acc: 0.507812]  [A loss: 1.021579, acc: 0.046875]\n",
            "960: [D loss: 0.705325, acc: 0.484375]  [A loss: 0.792496, acc: 0.218750]\n",
            "961: [D loss: 0.711649, acc: 0.511719]  [A loss: 0.969080, acc: 0.023438]\n",
            "962: [D loss: 0.677107, acc: 0.574219]  [A loss: 0.710723, acc: 0.480469]\n",
            "963: [D loss: 0.698619, acc: 0.521484]  [A loss: 0.925599, acc: 0.097656]\n",
            "964: [D loss: 0.674073, acc: 0.550781]  [A loss: 0.774623, acc: 0.304688]\n",
            "965: [D loss: 0.679469, acc: 0.542969]  [A loss: 0.945411, acc: 0.085938]\n",
            "966: [D loss: 0.681130, acc: 0.562500]  [A loss: 0.788841, acc: 0.292969]\n",
            "967: [D loss: 0.681443, acc: 0.541016]  [A loss: 0.890459, acc: 0.136719]\n",
            "968: [D loss: 0.679160, acc: 0.552734]  [A loss: 0.850711, acc: 0.195312]\n",
            "969: [D loss: 0.680883, acc: 0.568359]  [A loss: 0.873863, acc: 0.132812]\n",
            "970: [D loss: 0.693898, acc: 0.535156]  [A loss: 0.832182, acc: 0.234375]\n",
            "971: [D loss: 0.681675, acc: 0.554688]  [A loss: 0.857190, acc: 0.152344]\n",
            "972: [D loss: 0.682242, acc: 0.562500]  [A loss: 0.854298, acc: 0.148438]\n",
            "973: [D loss: 0.688061, acc: 0.550781]  [A loss: 0.862075, acc: 0.128906]\n",
            "974: [D loss: 0.666487, acc: 0.611328]  [A loss: 0.775323, acc: 0.269531]\n",
            "975: [D loss: 0.698306, acc: 0.533203]  [A loss: 0.995407, acc: 0.042969]\n",
            "976: [D loss: 0.668326, acc: 0.593750]  [A loss: 0.685394, acc: 0.519531]\n",
            "977: [D loss: 0.701443, acc: 0.521484]  [A loss: 1.026547, acc: 0.054688]\n",
            "978: [D loss: 0.684748, acc: 0.546875]  [A loss: 0.736207, acc: 0.437500]\n",
            "979: [D loss: 0.699133, acc: 0.529297]  [A loss: 0.962996, acc: 0.074219]\n",
            "980: [D loss: 0.674572, acc: 0.558594]  [A loss: 0.698841, acc: 0.523438]\n",
            "981: [D loss: 0.701695, acc: 0.535156]  [A loss: 0.963201, acc: 0.070312]\n",
            "982: [D loss: 0.666126, acc: 0.601562]  [A loss: 0.733810, acc: 0.394531]\n",
            "983: [D loss: 0.696644, acc: 0.535156]  [A loss: 0.961861, acc: 0.046875]\n",
            "984: [D loss: 0.686593, acc: 0.552734]  [A loss: 0.719864, acc: 0.457031]\n",
            "985: [D loss: 0.696617, acc: 0.531250]  [A loss: 0.920391, acc: 0.148438]\n",
            "986: [D loss: 0.667337, acc: 0.595703]  [A loss: 0.766633, acc: 0.312500]\n",
            "987: [D loss: 0.700200, acc: 0.531250]  [A loss: 0.914271, acc: 0.078125]\n",
            "988: [D loss: 0.671031, acc: 0.591797]  [A loss: 0.773622, acc: 0.335938]\n",
            "989: [D loss: 0.700527, acc: 0.511719]  [A loss: 0.903026, acc: 0.078125]\n",
            "990: [D loss: 0.686022, acc: 0.562500]  [A loss: 0.750300, acc: 0.332031]\n",
            "991: [D loss: 0.686255, acc: 0.566406]  [A loss: 0.904645, acc: 0.093750]\n",
            "992: [D loss: 0.682140, acc: 0.544922]  [A loss: 0.819400, acc: 0.218750]\n",
            "993: [D loss: 0.696565, acc: 0.531250]  [A loss: 0.876191, acc: 0.132812]\n",
            "994: [D loss: 0.687765, acc: 0.541016]  [A loss: 0.782774, acc: 0.285156]\n",
            "995: [D loss: 0.680947, acc: 0.572266]  [A loss: 0.863215, acc: 0.156250]\n",
            "996: [D loss: 0.678839, acc: 0.546875]  [A loss: 0.858718, acc: 0.179688]\n",
            "997: [D loss: 0.678640, acc: 0.587891]  [A loss: 0.810023, acc: 0.238281]\n",
            "998: [D loss: 0.675798, acc: 0.568359]  [A loss: 0.908893, acc: 0.125000]\n",
            "999: [D loss: 0.692370, acc: 0.531250]  [A loss: 0.811553, acc: 0.285156]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9rAFGEsbqUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "9b4eefda-61a6-4af7-b3ac-58da467b8e21"
      },
      "source": [
        "# Generate batch of synthetic MNIST images\n",
        "timer.elapsed_time()\n",
        "mnist_dcgan.plot_images(fake=True)\n",
        "mnist_dcgan.plot_images(fake=False, save2file=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed: 9.06054084300995 min \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAALICAYAAABiqwZ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9UgOuEEbsgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}