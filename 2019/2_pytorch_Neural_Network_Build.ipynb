{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-pytorch-Neural_Network_Build.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1dhiman/100days-ml/blob/master/2019/2_pytorch_Neural_Network_Build.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsBdEGzVRJql",
        "colab_type": "text"
      },
      "source": [
        "# Creating a Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGH2qNbFRDgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "train = datasets.MNIST('', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "\n",
        "test = datasets.MNIST('', train=False, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "\n",
        "\n",
        "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
        "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU_AQn8kRO9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGOT2PEoRfUH",
        "colab_type": "text"
      },
      "source": [
        "To make our model, we're going to create a class. We'll call this class `net` and this net will inhereit from the `nn.Module` class:\n",
        "\n",
        "#### Let's define our layers\n",
        "\n",
        "The `fc` just stands for fully connected. In our case, we have 4 layers. Each of our `nn.Linear` layers expects the first parameter to be the input size, and the 2nd parameter is the output size.\n",
        "\n",
        "So, our first layer takes in 28x28, because our images are 28x28 images of hand-drawn digits. Then this outputs 64 connections. This means the next layer, `fc2` takes in 64 (the next layer is always going to accept however many connections the previous layer outputs). From here, this layer ouputs 64, then `fc3` just does the same thing.\n",
        "\n",
        "`fc4` takes in 64, but outputs 10 because we have 10 classes.\n",
        "\n",
        "The simplest neural network is fully connected, and feed-forward, meaning we go from input to output. In one side and out the other in a \"forward\" manner. We do not have to do this, but, for this model, we will. So let's define a new method for this network called `forward` and then dictate how our data will pass through this model\n",
        "\n",
        "\n",
        "#### The `forward` method\n",
        "Notice the x is a parameter for the `forward` method. This will be our input data. As you can see, we literally just \"pass\" this data through the layers. \n",
        "\n",
        "We use activation functions to take the sum of the input data * weights, and then to determine if the neuron is firing or not. Currently, the most popular is the rectified linear, or **relu**, activation function. Basically, these activation functions are keeping our data scaled between 0 and 1.\n",
        "\n",
        "Finally, for the output layer, we're going to use **softmax**. Softmax makes sense to use for a multi-class problem, where each thing can only be one class or the other. This means the outputs themselves are a confidence score, adding up to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI2EoeIBRRkR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0c85d555-10c2-455e-8d51-0cbe0d0de4d7"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhDbu-urTzKh",
        "colab_type": "text"
      },
      "source": [
        "At this point, we've got a neural network that we can actually pass data to, and it will give us an output. Let's see. Let's just create a random image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UcxFojbRbUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.randn((28,28))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nlFC8cIT75G",
        "colab_type": "text"
      },
      "source": [
        "Our neural network wants this to be **flattened**, however so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrvzbRgaT4B6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X.view(-1,28*28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GooPQs6cUABd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = net(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dDViamzUNBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9ebecfc3-67df-4605-83be-38b06991f8a8"
      },
      "source": [
        "output"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.2319, -2.3906, -2.4500, -2.2204, -2.3358, -2.3962, -2.2876, -2.3282,\n",
              "         -2.2644, -2.1578]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkkcP-EfUZtT",
        "colab_type": "text"
      },
      "source": [
        "Of course, those outputs are pretty worthless to us. Instead, we want to iterate over our dataset as well as do our **backpropagation** next\n",
        "\n",
        "[Source](https://pythonprogramming.net/building-deep-learning-neural-network-pytorch/)"
      ]
    }
  ]
}